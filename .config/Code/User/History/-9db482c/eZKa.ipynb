{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8fd73a",
   "metadata": {},
   "source": [
    "# Feature & Target Pipeline\n",
    "Quick tests and evaluation on new targets/features/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b95c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/east/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and configuration ready\n",
      "=== Loading .hist_db_1h.csv ===\n",
      "\n",
      "Initial rows: 53,963\n",
      "\n",
      "=== FOUND ISSUES (prior to automated fixes) ===\n",
      "\n",
      "ðŸ”´ TEMPORAL: Missing hours: 1 cases\n",
      "  Missing timestamps sample:\n",
      "    2025-11-04 13:00:00\n",
      "\n",
      "ðŸ”´ DATA INTEGRITY: Identical consecutive OHLC rows: 174 cases\n",
      "  Sample cases:\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "  Affected dates (sample): 2020-01-02, 2020-01-03, 2020-01-04, 2020-01-05, 2020-01-06\n",
      "\n",
      "=== APPLYING AUTOMATED FIXES ===\n",
      "ACTION: Resampled/Reindexed to 53964 hourly intervals (was 53963).\n",
      "ACTION: Forward-filled NaNs after resampling. (5 NaNs potentially filled by ffill).\n",
      "\n",
      "=== FINAL STATUS (after automated fixes) ===\n",
      "DataFrame shape post-fixes: (53964, 5) (Original: (53963, 6))\n",
      "Date range: 2019-10-01 00:00:00 to 2025-11-26 11:00:00\n",
      "No null values remaining after fixes.\n",
      "Total null values remaining after fixes: 0\n",
      "Heavy cache ready: heavy_features_v1.pkl (total 1) in cache/heavy_features\n",
      "Loaded heavy cache payload from disk; skipping rebuild.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from data_pipeline import load_data  # This just loads the data and cleans it\n",
    "from featureEngineer import FeatureEngineer\n",
    "from targetEngineer import ExpirationTargetEngineer\n",
    "from ML_setup import CONFIG\n",
    "from ML_general_tools import *\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Imports and configuration ready\")\n",
    "\n",
    "# Build features, targets, and combined dataframe\n",
    "raw_history = load_data(CONFIG[\"data\"][\"path\"])\n",
    "history_slice = raw_history[:]\n",
    "\n",
    "feature_params = dict(CONFIG[\"features\"][\"params\"])\n",
    "heavy_cache_cfg = CONFIG[\"features\"].get(\"heavy_cache\", {})\n",
    "heavy_cache_root = Path(heavy_cache_cfg.get(\"directory\", \"cache/heavy_features\"))\n",
    "\n",
    "current_output_root_str = CONFIG[\"output\"][\"directory\"]\n",
    "current_output_root_path = Path(current_output_root_str)\n",
    "\n",
    "paths = {\n",
    "    \"root\": current_output_root_path,\n",
    "    \"feature_selection\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"features\"],\n",
    "    \"trained_models\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"models\"],\n",
    "    \"hpt_studies\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"hpt\"],\n",
    "    \"feature_cache\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"cache\"]\n",
    "}\n",
    "\n",
    "\n",
    "cache_dir = heavy_cache_root\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "cache_files = sorted(cache_dir.glob(\"heavy_features_v*.pkl\"))\n",
    "cache_ready = bool(cache_files)\n",
    "if cache_ready:\n",
    "    print(f\"Heavy cache ready: {cache_files[-1].name} (total {len(cache_files)}) in {cache_dir}\")\n",
    "else:\n",
    "    print(f\"No heavy cache file found in {cache_dir}; initial fit will populate.\")\n",
    "\n",
    "## Feture Engineering\n",
    "fe = FeatureEngineer(verbose=feature_params.get(\"verbose\", False), **{k: v for k, v in feature_params.items() if k != \"verbose\"})\n",
    "\n",
    "## cache\n",
    "cache_ready = True  # Force rebuild for testing\n",
    "\n",
    "manual_features = None\n",
    "if cache_ready and fe.heavy_cache.load():\n",
    "    print(\"Loaded heavy cache payload from disk; skipping rebuild.\")\n",
    "    fe._heavy_payload = fe.heavy_cache.payload\n",
    "    reference = fe._prepare_reference_frame(history_slice)\n",
    "    fe._full_reference = reference\n",
    "    manual_features = fe._compute_all_features(reference, build_heavy=False)\n",
    "    fe.feature_names_out_ = manual_features.columns.tolist()\n",
    "    fe._reference_features = manual_features\n",
    "    print(f\"Manual features shape from cache: {manual_features.shape}\")\n",
    "else:\n",
    "    print(\"Heavy cache not available or failed to load; running full fit.\")\n",
    "    verbose_flag = feature_params.pop(\"verbose\", False)\n",
    "    fe = FeatureEngineer(verbose=verbose_flag, **feature_params)\n",
    "    fe.fit(history_slice)\n",
    "    manual_features = fe.transform(history_slice)\n",
    "\n",
    "feature_engineer = fe\n",
    "features = manual_features.copy()\n",
    "\n",
    "## Add targets standard expiration targets\n",
    "# target_engineer = ExpirationTargetEngineer(**CONFIG[\"targets\"][\"params\"])\n",
    "# target_engineer.fit(features)\n",
    "# targets = target_engineer.transform(features)\n",
    "\n",
    "## 2a. Volatility Regime Target Engineering Test ---\n",
    "from targetEngineer import VolatilityRegimeEngineer\n",
    "\n",
    "df_train = features.copy()\n",
    "\n",
    "# Instantiate with your parameters\n",
    "regime_engineer = VolatilityRegimeEngineer(\n",
    "    lookback_window=24*3,    # 3 days lookback for vol\n",
    "    seasonal_window=24*30,   # 30 days to learn patterns\n",
    "    forward_window=24,       # 24h classification\n",
    "    trend_std=1.2,           # 1.2 daily sigmas\n",
    "    jump_std=3.0,            # 3.0 daily sigmas (scaled internally for 6h window)\n",
    "    jump_speed_window=6,     # 6h window for jump detection\n",
    "\n",
    "    # Hardening Parameters\n",
    "    trend_min_efficiency=0.15, # Allows looser/messier trends\n",
    "    trend_min_r2=0.6           # Requires moderate linear fit\n",
    ")\n",
    "\n",
    "# Run Fit/Transform\n",
    "regime_engineer.fit(df_train)\n",
    "targets = regime_engineer.transform(df_train)\n",
    "\n",
    "# Check the distribution\n",
    "dist = regime_engineer.get_regime_distribution(df_train)\n",
    "print(dist)\n",
    "\n",
    "\n",
    "initial_feature_names = list(features.columns)\n",
    "\n",
    "# Generate targets (your existing logic)\n",
    "print(initial_feature_names)\n",
    "\n",
    "\n",
    "## deop price columns from features\n",
    "# drop_cols = [col for col in ['o', 'h', 'l', 'c', 'volCcy'] if col in features.columns]\n",
    "# if drop_cols: features = features.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat([features, targets], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030f489c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Find rows with any NaN values in features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nan_mask = \u001b[43mfeatures\u001b[49m.isna().any(axis=\u001b[32m1\u001b[39m)\n\u001b[32m      3\u001b[39m features_with_nans = features[nan_mask]\n",
      "\u001b[31mNameError\u001b[39m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find rows with any NaN values in features\n",
    "nan_mask = features.isna().any(axis=1)\n",
    "features_with_nans = features[nan_mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea14ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Clean combined dataframe - drop first month and last 11 rows (minimal cleaning for small dataset)\n",
    "months_to_drop = 1  # Only 1 month for small dataset\n",
    "tail_rows_to_drop = 11\n",
    "\n",
    "cutoff = combined_df.index.min() + pd.DateOffset(months=months_to_drop)\n",
    "print(f\"Removing data before {cutoff:%Y-%m-%d} (first {months_to_drop} months)\")\n",
    "combined_df_clean = combined_df.loc[combined_df.index >= cutoff]\n",
    "\n",
    "if tail_rows_to_drop > 0:\n",
    "    print(f\"Dropping last {tail_rows_to_drop} rows to avoid trailing NaNs\")\n",
    "    combined_df_clean = combined_df_clean.iloc[:-tail_rows_to_drop]\n",
    "\n",
    "print(f\"Rows after cleaning: {len(combined_df_clean)} (from {combined_df_clean.index[0]} to {combined_df_clean.index[-1]})\")\n",
    "\n",
    "# Split into train/val/test (80/10/10)\n",
    "n_samples = len(combined_df_clean)\n",
    "train_end = int(n_samples * 0.8)\n",
    "val_end = train_end + int(n_samples * 0.1)\n",
    "\n",
    "# Get feature and target columns\n",
    "feature_cols = features.columns.intersection(combined_df_clean.columns)\n",
    "target_cols = targets.columns.intersection(combined_df_clean.columns)\n",
    "\n",
    "X_train = combined_df_clean[feature_cols].iloc[:train_end]\n",
    "X_val = combined_df_clean[feature_cols].iloc[train_end:val_end]\n",
    "X_test = combined_df_clean[feature_cols].iloc[val_end:]\n",
    "\n",
    "y_train = combined_df_clean[target_cols].iloc[:train_end]\n",
    "y_val = combined_df_clean[target_cols].iloc[train_end:val_end]\n",
    "y_test = combined_df_clean[target_cols].iloc[val_end:]\n",
    "\n",
    "print(f\"\\nX shapes -> train {X_train.shape}, val {X_val.shape}, test {X_test.shape}\")\n",
    "print(f\"y shapes -> train {y_train.shape}, val {y_val.shape}, test {y_test.shape}\")\n",
    "\n",
    "# Quick NaN check on training data\n",
    "train_nans = X_train.isna().sum()\n",
    "if train_nans.sum() > 0:\n",
    "    print(f\"\\nâš  Training features with NaNs: {(train_nans > 0).sum()} columns\")\n",
    "    print(f\"  Max NaNs in any column: {train_nans.max()} ({train_nans.max()/len(X_train):.1%})\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No NaNs in training features\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8fd73a",
   "metadata": {},
   "source": [
    "# Feature & Target Pipeline\n",
    "Quick tests and evaluation on new targets/features/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64b95c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and configuration ready\n",
      "=== Loading .hist_db_1h.csv ===\n",
      "\n",
      "Initial rows: 53,963\n",
      "\n",
      "=== FOUND ISSUES (prior to automated fixes) ===\n",
      "\n",
      "ðŸ”´ TEMPORAL: Missing hours: 1 cases\n",
      "  Missing timestamps sample:\n",
      "    2025-11-04 13:00:00\n",
      "\n",
      "ðŸ”´ DATA INTEGRITY: Identical consecutive OHLC rows: 174 cases\n",
      "  Sample cases:\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "  Affected dates (sample): 2020-01-02, 2020-01-03, 2020-01-04, 2020-01-05, 2020-01-06\n",
      "\n",
      "=== APPLYING AUTOMATED FIXES ===\n",
      "ACTION: Resampled/Reindexed to 53964 hourly intervals (was 53963).\n",
      "ACTION: Forward-filled NaNs after resampling. (5 NaNs potentially filled by ffill).\n",
      "\n",
      "=== FINAL STATUS (after automated fixes) ===\n",
      "DataFrame shape post-fixes: (53964, 5) (Original: (53963, 6))\n",
      "Date range: 2019-10-01 00:00:00 to 2025-11-26 11:00:00\n",
      "No null values remaining after fixes.\n",
      "Total null values remaining after fixes: 0\n",
      "Loaded raw data: (53964, 5) in 0.07s\n",
      "Using slice: (53964, 5)\n",
      "Heavy cache ready: heavy_features_v1.pkl (total 1) in cache/heavy_features\n",
      "\n",
      "âœ“ Using heavy cache (only prev_cycle features cached)\n",
      "  Note: Rolling/stateless features still computed on-the-fly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1582: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_sin\"] = df[vol_feat] * df[\"tte_phase_sin\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1584: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_cos\"] = df[vol_feat] * df[\"tte_phase_cos\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1572: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_sqrt\"] = df[vol_feat] * tte_sqrt\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1575: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte\"] = df[vol_feat] * tte\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1578: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_sq\"] = df[vol_feat] * (tte_normalized ** 2)\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1582: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_sin\"] = df[vol_feat] * df[\"tte_phase_sin\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1584: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_cos\"] = df[vol_feat] * df[\"tte_phase_cos\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1589: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vol_term_x_tte_sqrt\"] = vol_term_slope * tte_sqrt  # Black-Scholes scaling\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1590: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vol_term_x_tte\"] = vol_term_slope * tte\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1591: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vol_term_x_tte_sq\"] = vol_term_slope * (tte_normalized ** 2)\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1605: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekend\"] = df[vol_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1606: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekday\"] = df[vol_feat] * (1 - df[\"is_weekend\"])\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1605: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekend\"] = df[vol_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1606: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekday\"] = df[vol_feat] * (1 - df[\"is_weekend\"])\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1605: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekend\"] = df[vol_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1606: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekday\"] = df[vol_feat] * (1 - df[\"is_weekend\"])\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1605: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekend\"] = df[vol_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1606: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekday\"] = df[vol_feat] * (1 - df[\"is_weekend\"])\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1620: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"volume_weekend_effect\"] = df[\"vlm_ma_24h\"] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{cycle_type}_range_x_vol\"] = cycle_range * df[\"vol_gkyz_12h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{cycle_type}_range_x_vol\"] = cycle_range * df[\"vol_gkyz_12h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{cycle_type}_range_x_vol\"] = cycle_range * df[\"vol_gkyz_12h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1658: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"weekday_vs_saturday_prog\"] = df[\"prev_weekday_ProgActP\"] - df[\"prev_saturday_ProgActP\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1661: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"weekday_vs_sunday_prog\"] = df[\"prev_weekday_ProgActP\"] - df[\"prev_sunday_ProgActP\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1665: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"prev_cycle_progress_x_hour\"] = df[\"prev_weekday_ProgActP\"] * df[\"hour_of_week_sin\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1674: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"extreme_range_vol\"] = df[\"compressed_range_vol\"] * df[\"vol_gkyz_3h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1678: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"skew_vol_extreme\"] = df[\"returns_skew_24h\"] * df[\"vol_gkyz_6h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1682: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"kurtosis_vol_extreme\"] = df[\"returns_kurtosis_24h\"] * df[\"vol_gkyz_12h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1686: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"distance_vol_extreme\"] = df[\"dist_from_high_144h\"] * df[\"vol_gkyz_24h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1690: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vol_surprise_clustering\"] = df[\"volume_surprise\"] * df[\"vol_clustering\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FeatureEngineer] feature build complete; rows=53964, cols=450, total=10.64s [stateless:234.9ms, merge_stateless:4.3ms, temporal:12.6ms, rolling:762.1ms, prev_week_cycle:44.5ms, current_cycle:9125.5ms, non_linear:250.3ms, custom_interactions:27.6ms, cleanup:177.9ms]\n",
      "  Features computed in 10.64s -> shape: (53964, 450)\n",
      "\n",
      "--- Building Volatility Regime Targets ---\n",
      "Regime targets built in 341.23s -> shape: (53964, 6)\n",
      "Regime targets built in 341.23s -> shape: (53964, 6)\n",
      "\n",
      "Regime distribution:\n",
      "regime_label\n",
      "0    43673\n",
      "1     6364\n",
      "2     3795\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Combined shape: (53964, 456)\n",
      "Total pipeline time: 689.30s\n",
      "\n",
      "Regime distribution:\n",
      "regime_label\n",
      "0    43673\n",
      "1     6364\n",
      "2     3795\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Combined shape: (53964, 456)\n",
      "Total pipeline time: 689.30s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import time\n",
    "from data_pipeline import load_data  # This just loads the data and cleans it\n",
    "from featureEngineer import FeatureEngineer\n",
    "from targetEngineer import ExpirationTargetEngineer\n",
    "from ML_setup import CONFIG\n",
    "from ML_general_tools import *\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Imports and configuration ready\")\n",
    "\n",
    "# Build features, targets, and combined dataframe\n",
    "t0 = time.time()\n",
    "raw_history = load_data(CONFIG[\"data\"][\"path\"])\n",
    "print(f\"Loaded raw data: {raw_history.shape} in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# Use slice for faster testing (or use [:] for full data)\n",
    "history_slice = raw_history[:]  # Last 3000 rows for faster testing\n",
    "print(f\"Using slice: {history_slice.shape}\")\n",
    "\n",
    "feature_params = dict(CONFIG[\"features\"][\"params\"])\n",
    "heavy_cache_cfg = CONFIG[\"features\"].get(\"heavy_cache\", {})\n",
    "heavy_cache_root = Path(heavy_cache_cfg.get(\"directory\", \"cache/heavy_features\"))\n",
    "\n",
    "current_output_root_str = CONFIG[\"output\"][\"directory\"]\n",
    "current_output_root_path = Path(current_output_root_str)\n",
    "\n",
    "paths = {\n",
    "    \"root\": current_output_root_path,\n",
    "    \"feature_selection\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"features\"],\n",
    "    \"trained_models\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"models\"],\n",
    "    \"hpt_studies\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"hpt\"],\n",
    "    \"feature_cache\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"cache\"]\n",
    "}\n",
    "\n",
    "cache_dir = heavy_cache_root\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "cache_files = sorted(cache_dir.glob(\"heavy_features_v*.pkl\"))\n",
    "cache_ready = bool(cache_files)\n",
    "if cache_ready:\n",
    "    print(f\"Heavy cache ready: {cache_files[-1].name} (total {len(cache_files)}) in {cache_dir}\")\n",
    "else:\n",
    "    print(f\"No heavy cache file found in {cache_dir}; initial fit will populate.\")\n",
    "\n",
    "## Feature Engineering\n",
    "fe = FeatureEngineer(verbose=True, **{k: v for k, v in feature_params.items() if k != \"verbose\"})\n",
    "\n",
    "## Cache usage\n",
    "cache_ready = bool(cache_files)  # Use actual cache status\n",
    "\n",
    "manual_features = None\n",
    "if cache_ready and fe.heavy_cache.load():\n",
    "    print(\"\\nâœ“ Using heavy cache (only prev_cycle features cached)\")\n",
    "    print(\"  Note: Rolling/stateless features still computed on-the-fly\")\n",
    "    t1 = time.time()\n",
    "    fe._heavy_payload = fe.heavy_cache.payload\n",
    "    reference = fe._prepare_reference_frame(history_slice)\n",
    "    fe._full_reference = reference\n",
    "    manual_features = fe._compute_all_features(reference, build_heavy=False)\n",
    "    fe.feature_names_out_ = manual_features.columns.tolist()\n",
    "    fe._reference_features = manual_features\n",
    "    print(f\"  Features computed in {time.time()-t1:.2f}s -> shape: {manual_features.shape}\")\n",
    "else:\n",
    "    print(\"\\nâš  Heavy cache not available; running full fit (slower)\")\n",
    "    t1 = time.time()\n",
    "    verbose_flag = feature_params.pop(\"verbose\", False)\n",
    "    fe = FeatureEngineer(verbose=True, **feature_params)\n",
    "    fe.fit(history_slice)\n",
    "    manual_features = fe.transform(history_slice)\n",
    "    print(f\"  Full fit+transform in {time.time()-t1:.2f}s -> shape: {manual_features.shape}\")\n",
    "\n",
    "feature_engineer = fe\n",
    "features = manual_features.copy()\n",
    "\n",
    "## 2a. Volatility Regime Target Engineering ---\n",
    "from targetEngineer import VolatilityRegimeEngineer\n",
    "\n",
    "print(\"\\n--- Building Volatility Regime Targets ---\")\n",
    "t2 = time.time()\n",
    "\n",
    "regime_engineer = VolatilityRegimeEngineer(\n",
    "    lookback_window=24*3,    # 3 days lookback for vol\n",
    "    seasonal_window=24*30,   # 30 days to learn patterns\n",
    "    forward_window=24,       # 24h classification\n",
    "    trend_std=1.2,           # 1.2 daily sigmas\n",
    "    jump_std=3.0,            # 3.0 daily sigmas\n",
    "    jump_speed_window=6,     # 6h window for jump detection\n",
    ")\n",
    "\n",
    "regime_engineer.fit(features)\n",
    "targets = regime_engineer.transform(features)\n",
    "print(f\"Regime targets built in {time.time()-t2:.2f}s -> shape: {targets.shape}\")\n",
    "\n",
    "# Check distribution\n",
    "dist = regime_engineer.get_regime_distribution(features)\n",
    "print(\"\\nRegime distribution:\")\n",
    "print(dist)\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat([features, targets], axis=1)\n",
    "print(f\"\\nCombined shape: {combined_df.shape}\")\n",
    "print(f\"Total pipeline time: {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cache not found or FORCE_REBUILD=True - will save after first run\n",
      "To use cache next time:\n",
      "  1. Run the first cell with history_slice = raw_history[:]\n",
      "  2. Wait for features/targets to compute\n",
      "  3. This cell will save them\n",
      "  4. Next time, set FORCE_REBUILD=False and skip the first cell\n",
      "============================================================\n",
      "\n",
      "Saving current features and targets to cache...\n",
      "âœ“ Saved to cache in 0.36s\n",
      "  Location: research_vol\n",
      "âœ“ Saved to cache in 0.36s\n",
      "  Location: research_vol\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Define cache paths\n",
    "cache_root = paths[\"root\"]\n",
    "cache_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "feature_cache = cache_root / \"features_cache.pkl\"\n",
    "target_cache = cache_root / \"targets_cache.pkl\"\n",
    "combined_cache = cache_root / \"combined_cache.pkl\"\n",
    "\n",
    "# Option 1: Load from cache if exists\n",
    "FORCE_REBUILD = False  # Set to True to rebuild from scratch\n",
    "\n",
    "if not FORCE_REBUILD and feature_cache.exists() and target_cache.exists():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Loading cached features and targets...\")\n",
    "    t_load = time.time()\n",
    "    \n",
    "    with open(feature_cache, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    with open(target_cache, 'rb') as f:\n",
    "        targets = pickle.load(f)\n",
    "    with open(combined_cache, 'rb') as f:\n",
    "        combined_df = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded from cache in {time.time()-t_load:.2f}s\")\n",
    "    print(f\"  Features: {features.shape}\")\n",
    "    print(f\"  Targets: {targets.shape}\")\n",
    "    print(f\"  Combined: {combined_df.shape}\")\n",
    "    print(f\"  Date range: {features.index[0]} to {features.index[-1]}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Cache not found or FORCE_REBUILD=True - will save after first run\")\n",
    "    print(\"To use cache next time:\")\n",
    "    print(\"  1. Run the first cell with history_slice = raw_history[:]\")\n",
    "    print(\"  2. Wait for features/targets to compute\")\n",
    "    print(\"  3. This cell will save them\")\n",
    "    print(\"  4. Next time, set FORCE_REBUILD=False and skip the first cell\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save the current run to cache\n",
    "    if 'features' in globals() and 'targets' in globals():\n",
    "        print(\"\\nSaving current features and targets to cache...\")\n",
    "        t_save = time.time()\n",
    "        \n",
    "        with open(feature_cache, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "        with open(target_cache, 'wb') as f:\n",
    "            pickle.dump(targets, f)\n",
    "        with open(combined_cache, 'wb') as f:\n",
    "            pickle.dump(combined_df, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved to cache in {time.time()-t_save:.2f}s\")\n",
    "        print(f\"  Location: {cache_root}\")\n",
    "    else:\n",
    "        print(\"âš  No features/targets to save yet - run the first cell first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea14ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_457419/2954929995.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  features[100:][nan_mask]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>c</th>\n",
       "      <th>volCcy</th>\n",
       "      <th>time_to_exp1_hr</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_saturday_range_x_vol</th>\n",
       "      <th>prev_sunday_range_x_vol</th>\n",
       "      <th>weekday_vs_saturday_prog</th>\n",
       "      <th>weekday_vs_sunday_prog</th>\n",
       "      <th>prev_cycle_progress_x_hour</th>\n",
       "      <th>extreme_range_vol</th>\n",
       "      <th>skew_vol_extreme</th>\n",
       "      <th>kurtosis_vol_extreme</th>\n",
       "      <th>distance_vol_extreme</th>\n",
       "      <th>vol_surprise_clustering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-05 04:00:00</th>\n",
       "      <td>8149.4</td>\n",
       "      <td>8156.7</td>\n",
       "      <td>8139.9</td>\n",
       "      <td>8139.9</td>\n",
       "      <td>370.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>-0.000906</td>\n",
       "      <td>-0.000241</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>-0.195495</td>\n",
       "      <td>-0.766529</td>\n",
       "      <td>0.017159</td>\n",
       "      <td>-4.370304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-05 05:00:00</th>\n",
       "      <td>8139.9</td>\n",
       "      <td>8140.0</td>\n",
       "      <td>8124.2</td>\n",
       "      <td>8128.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>-0.197283</td>\n",
       "      <td>-0.751756</td>\n",
       "      <td>0.015109</td>\n",
       "      <td>-3.933097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-05 06:00:00</th>\n",
       "      <td>8128.0</td>\n",
       "      <td>8145.4</td>\n",
       "      <td>8115.3</td>\n",
       "      <td>8142.3</td>\n",
       "      <td>615.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>-0.000522</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>-0.163173</td>\n",
       "      <td>-0.746458</td>\n",
       "      <td>0.015558</td>\n",
       "      <td>-3.672246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-05 07:00:00</th>\n",
       "      <td>8142.3</td>\n",
       "      <td>8142.3</td>\n",
       "      <td>8112.6</td>\n",
       "      <td>8123.3</td>\n",
       "      <td>844.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>-0.164850</td>\n",
       "      <td>-0.763307</td>\n",
       "      <td>0.014923</td>\n",
       "      <td>-1.011547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-05 08:00:00</th>\n",
       "      <td>8123.3</td>\n",
       "      <td>8123.3</td>\n",
       "      <td>8081.0</td>\n",
       "      <td>8102.8</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>-0.182369</td>\n",
       "      <td>-0.680221</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.973009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-18 14:00:00</th>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.317772</td>\n",
       "      <td>0.014483</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-18 15:00:00</th>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>-0.000350</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.316647</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-18 16:00:00</th>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.186966</td>\n",
       "      <td>0.013975</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-18 17:00:00</th>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>16739.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.087721</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-18 18:00:00</th>\n",
       "      <td>16733.8</td>\n",
       "      <td>16746.7</td>\n",
       "      <td>16719.3</td>\n",
       "      <td>16743.8</td>\n",
       "      <td>92.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.040372</td>\n",
       "      <td>-0.119361</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows Ã— 450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           o        h        l        c  volCcy  \\\n",
       "2019-10-05 04:00:00   8149.4   8156.7   8139.9   8139.9   370.0   \n",
       "2019-10-05 05:00:00   8139.9   8140.0   8124.2   8128.0   355.0   \n",
       "2019-10-05 06:00:00   8128.0   8145.4   8115.3   8142.3   615.0   \n",
       "2019-10-05 07:00:00   8142.3   8142.3   8112.6   8123.3   844.0   \n",
       "2019-10-05 08:00:00   8123.3   8123.3   8081.0   8102.8  1864.0   \n",
       "...                      ...      ...      ...      ...     ...   \n",
       "2022-12-18 14:00:00  16739.7  16739.7  16739.7  16739.7     0.0   \n",
       "2022-12-18 15:00:00  16739.7  16739.7  16739.7  16739.7     0.0   \n",
       "2022-12-18 16:00:00  16739.7  16739.7  16739.7  16739.7     0.0   \n",
       "2022-12-18 17:00:00  16739.7  16739.7  16739.7  16739.7     0.0   \n",
       "2022-12-18 18:00:00  16733.8  16746.7  16719.3  16743.8    92.0   \n",
       "\n",
       "                     time_to_exp1_hr  time_elapsed  hour  day_of_week  \\\n",
       "2019-10-05 04:00:00              3.0          21.0     5            5   \n",
       "2019-10-05 05:00:00              2.0          22.0     6            5   \n",
       "2019-10-05 06:00:00              1.0          23.0     7            5   \n",
       "2019-10-05 07:00:00             24.0           0.0     8            5   \n",
       "2019-10-05 08:00:00             23.0           1.0     9            5   \n",
       "...                              ...           ...   ...          ...   \n",
       "2022-12-18 14:00:00             17.0           7.0    15            6   \n",
       "2022-12-18 15:00:00             16.0           8.0    16            6   \n",
       "2022-12-18 16:00:00             15.0           9.0    17            6   \n",
       "2022-12-18 17:00:00             14.0          10.0    18            6   \n",
       "2022-12-18 18:00:00             13.0          11.0    19            6   \n",
       "\n",
       "                     is_weekend  ...  prev_saturday_range_x_vol  \\\n",
       "2019-10-05 04:00:00           1  ...                   0.005596   \n",
       "2019-10-05 05:00:00           1  ...                   0.005495   \n",
       "2019-10-05 06:00:00           1  ...                   0.005467   \n",
       "2019-10-05 07:00:00           1  ...                   0.000000   \n",
       "2019-10-05 08:00:00           1  ...                   0.001047   \n",
       "...                         ...  ...                        ...   \n",
       "2022-12-18 14:00:00           1  ...                   0.001108   \n",
       "2022-12-18 15:00:00           1  ...                   0.001227   \n",
       "2022-12-18 16:00:00           1  ...                   0.000804   \n",
       "2022-12-18 17:00:00           1  ...                   0.000404   \n",
       "2022-12-18 18:00:00           1  ...                   0.000572   \n",
       "\n",
       "                     prev_sunday_range_x_vol  weekday_vs_saturday_prog  \\\n",
       "2019-10-05 04:00:00                 0.008488                 -0.000906   \n",
       "2019-10-05 05:00:00                 0.008589                  0.000240   \n",
       "2019-10-05 06:00:00                 0.008732                 -0.000522   \n",
       "2019-10-05 07:00:00                 0.000000                  0.000000   \n",
       "2019-10-05 08:00:00                 0.001030                  0.000259   \n",
       "...                                      ...                       ...   \n",
       "2022-12-18 14:00:00                 0.001329                 -0.000538   \n",
       "2022-12-18 15:00:00                 0.001406                 -0.000350   \n",
       "2022-12-18 16:00:00                 0.000895                 -0.000991   \n",
       "2022-12-18 17:00:00                 0.000451                  0.000272   \n",
       "2022-12-18 18:00:00                 0.000646                 -0.000310   \n",
       "\n",
       "                     weekday_vs_sunday_prog  prev_cycle_progress_x_hour  \\\n",
       "2019-10-05 04:00:00               -0.000241                    0.000108   \n",
       "2019-10-05 05:00:00                0.000740                   -0.000755   \n",
       "2019-10-05 06:00:00                0.000732                   -0.000281   \n",
       "2019-10-05 07:00:00                0.000000                   -0.000000   \n",
       "2019-10-05 08:00:00               -0.000048                    0.000060   \n",
       "...                                     ...                         ...   \n",
       "2022-12-18 14:00:00                0.000193                    0.000034   \n",
       "2022-12-18 15:00:00                0.000076                   -0.000000   \n",
       "2022-12-18 16:00:00               -0.000439                    0.000113   \n",
       "2022-12-18 17:00:00                0.000524                   -0.000120   \n",
       "2022-12-18 18:00:00               -0.000046                   -0.000030   \n",
       "\n",
       "                     extreme_range_vol  skew_vol_extreme  \\\n",
       "2019-10-05 04:00:00           0.003825         -0.195495   \n",
       "2019-10-05 05:00:00           0.002073         -0.197283   \n",
       "2019-10-05 06:00:00           0.002731         -0.163173   \n",
       "2019-10-05 07:00:00           0.004386         -0.164850   \n",
       "2019-10-05 08:00:00           0.006320         -0.182369   \n",
       "...                                ...               ...   \n",
       "2022-12-18 14:00:00           0.000000         -0.000000   \n",
       "2022-12-18 15:00:00           0.000000         -0.000000   \n",
       "2022-12-18 16:00:00           0.000000         -0.000000   \n",
       "2022-12-18 17:00:00           0.000000         -0.000000   \n",
       "2022-12-18 18:00:00           0.000000         -0.040372   \n",
       "\n",
       "                     kurtosis_vol_extreme  distance_vol_extreme  \\\n",
       "2019-10-05 04:00:00             -0.766529              0.017159   \n",
       "2019-10-05 05:00:00             -0.751756              0.015109   \n",
       "2019-10-05 06:00:00             -0.746458              0.015558   \n",
       "2019-10-05 07:00:00             -0.763307              0.014923   \n",
       "2019-10-05 08:00:00             -0.680221              0.015917   \n",
       "...                                   ...                   ...   \n",
       "2022-12-18 14:00:00             -0.317772              0.014483   \n",
       "2022-12-18 15:00:00             -0.316647              0.014138   \n",
       "2022-12-18 16:00:00             -0.186966              0.013975   \n",
       "2022-12-18 17:00:00             -0.087721              0.013408   \n",
       "2022-12-18 18:00:00             -0.119361              0.012950   \n",
       "\n",
       "                     vol_surprise_clustering  \n",
       "2019-10-05 04:00:00                -4.370304  \n",
       "2019-10-05 05:00:00                -3.933097  \n",
       "2019-10-05 06:00:00                -3.672246  \n",
       "2019-10-05 07:00:00                -1.011547  \n",
       "2019-10-05 08:00:00                 0.973009  \n",
       "...                                      ...  \n",
       "2022-12-18 14:00:00                -1.000000  \n",
       "2022-12-18 15:00:00                -1.000000  \n",
       "2022-12-18 16:00:00                -1.000000  \n",
       "2022-12-18 17:00:00                -1.000000  \n",
       "2022-12-18 18:00:00                -1.000000  \n",
       "\n",
       "[360 rows x 450 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprehensive NaN analysis in features\n",
    "print(\"=\" * 70)\n",
    "print(\"NaN Analysis in Features\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Overall NaN statistics\n",
    "nan_counts = features.isna().sum()\n",
    "nan_features = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nTotal features: {len(features.columns)}\")\n",
    "print(f\"Features with NaNs: {len(nan_features)}\")\n",
    "print(f\"Total rows: {len(features)}\")\n",
    "\n",
    "# 2. Group NaN features by prefix to identify source\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NaN Features Grouped by Source:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_groups = {}\n",
    "for feat in nan_features.index:\n",
    "    # Extract prefix (everything before first underscore or digit)\n",
    "    if '_' in feat:\n",
    "        prefix = feat.split('_')[0]\n",
    "    else:\n",
    "        prefix = 'other'\n",
    "    \n",
    "    if prefix not in feature_groups:\n",
    "        feature_groups[prefix] = []\n",
    "    feature_groups[prefix].append((feat, nan_counts[feat]))\n",
    "\n",
    "# Sort groups by total NaN count\n",
    "sorted_groups = sorted(feature_groups.items(), \n",
    "                       key=lambda x: sum(count for _, count in x[1]), \n",
    "                       reverse=True)\n",
    "\n",
    "for prefix, features_list in sorted_groups[:10]:  # Top 10 groups\n",
    "    total_nans = sum(count for _, count in features_list)\n",
    "    print(f\"\\n{prefix.upper()} features: {len(features_list)} features, {total_nans:,} total NaNs\")\n",
    "    # Show top 5 within each group\n",
    "    for feat, count in sorted(features_list, key=lambda x: x[1], reverse=True)[:5]:\n",
    "        pct = (count / len(features)) * 100\n",
    "        print(f\"  {feat:50s} {count:6,} NaNs ({pct:5.2f}%)\")\n",
    "\n",
    "# 3. Analyze NaN patterns (start/middle/end)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NaN Location Analysis (Top 10 worst features):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for feat in nan_features.head(10).index:\n",
    "    series = features[feat]\n",
    "    nan_mask = series.isna()\n",
    "    \n",
    "    # Find first and last valid index\n",
    "    valid_indices = series[~nan_mask].index\n",
    "    if len(valid_indices) == 0:\n",
    "        print(f\"\\n{feat}: ALL NaNs!\")\n",
    "        continue\n",
    "    \n",
    "    first_valid = valid_indices[0]\n",
    "    last_valid = valid_indices[-1]\n",
    "    \n",
    "    # Count NaNs at start, middle, end\n",
    "    start_nans = nan_mask.loc[:first_valid].sum() - 1  # -1 to exclude first valid\n",
    "    end_nans = nan_mask.loc[last_valid:].sum() - 1  # -1 to exclude last valid\n",
    "    middle_nans = nan_mask.sum() - start_nans - end_nans\n",
    "    \n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(f\"  Total NaNs: {nan_mask.sum():,} ({nan_mask.sum()/len(features)*100:.2f}%)\")\n",
    "    print(f\"  Start NaNs: {start_nans:,} (before {first_valid})\")\n",
    "    print(f\"  Middle NaNs: {middle_nans:,}\")\n",
    "    print(f\"  End NaNs: {end_nans:,} (after {last_valid})\")\n",
    "\n",
    "# 4. Check specific feature types that are expected\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Expected NaN Sources (prev_weekend, empirical, etc.):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prev_weekend_features = [f for f in nan_features.index if 'prev_saturday' in f or 'prev_sunday' in f]\n",
    "empirical_features = [f for f in nan_features.index if 'emp_' in f]\n",
    "prev_weekday_features = [f for f in nan_features.index if 'prev_weekday' in f]\n",
    "\n",
    "print(f\"\\nprev_saturday/sunday features with NaNs: {len(prev_weekend_features)}\")\n",
    "if prev_weekend_features:\n",
    "    for feat in prev_weekend_features[:5]:\n",
    "        print(f\"  {feat}: {nan_counts[feat]:,} NaNs\")\n",
    "\n",
    "print(f\"\\nemp_ (empirical) features with NaNs: {len(empirical_features)}\")\n",
    "if empirical_features:\n",
    "    for feat in empirical_features[:5]:\n",
    "        print(f\"  {feat}: {nan_counts[feat]:,} NaNs\")\n",
    "\n",
    "print(f\"\\nprev_weekday features with NaNs: {len(prev_weekday_features)}\")\n",
    "if prev_weekday_features:\n",
    "    for feat in prev_weekday_features[:5]:\n",
    "        print(f\"  {feat}: {nan_counts[feat]:,} NaNs\")\n",
    "\n",
    "# 5. Check which rows have NaNs\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Row-wise NaN Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rows_with_nans = features.isna().any(axis=1)\n",
    "print(f\"Rows with ANY NaNs: {rows_with_nans.sum():,} / {len(features):,} ({rows_with_nans.sum()/len(features)*100:.2f}%)\")\n",
    "\n",
    "# Show first and last rows with NaNs\n",
    "nan_row_indices = features[rows_with_nans].index\n",
    "if len(nan_row_indices) > 0:\n",
    "    print(f\"First row with NaNs: {nan_row_indices[0]}\")\n",
    "    print(f\"Last row with NaNs: {nan_row_indices[-1]}\")\n",
    "    \n",
    "    # Count consecutive NaNs at start and end\n",
    "    consecutive_start = 0\n",
    "    for i in range(len(rows_with_nans)):\n",
    "        if rows_with_nans.iloc[i]:\n",
    "            consecutive_start += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    consecutive_end = 0\n",
    "    for i in range(len(rows_with_nans)-1, -1, -1):\n",
    "        if rows_with_nans.iloc[i]:\n",
    "            consecutive_end += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(f\"Consecutive NaN rows at start: {consecutive_start}\")\n",
    "    print(f\"Consecutive NaN rows at end: {consecutive_end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8f63c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing data before 2019-11-01 (first 1 months)\n",
      "NaN rows after cutoff removal: 173 / 53220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>c</th>\n",
       "      <th>volCcy</th>\n",
       "      <th>time_to_exp1_hr</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>skew_vol_extreme</th>\n",
       "      <th>kurtosis_vol_extreme</th>\n",
       "      <th>distance_vol_extreme</th>\n",
       "      <th>vol_surprise_clustering</th>\n",
       "      <th>regime_label</th>\n",
       "      <th>max_fwd_z_score</th>\n",
       "      <th>max_jump_z_score</th>\n",
       "      <th>box_std_deseasonalized</th>\n",
       "      <th>box_std_raw</th>\n",
       "      <th>seasonal_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-01 00:00:00</th>\n",
       "      <td>9151.2</td>\n",
       "      <td>9155.8</td>\n",
       "      <td>9115.0</td>\n",
       "      <td>9135.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.088773</td>\n",
       "      <td>0.125074</td>\n",
       "      <td>0.121527</td>\n",
       "      <td>-4.691296</td>\n",
       "      <td>0</td>\n",
       "      <td>0.572082</td>\n",
       "      <td>1.144163</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 01:00:00</th>\n",
       "      <td>9135.0</td>\n",
       "      <td>9149.0</td>\n",
       "      <td>9108.8</td>\n",
       "      <td>9108.8</td>\n",
       "      <td>987.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.117588</td>\n",
       "      <td>0.127607</td>\n",
       "      <td>0.122305</td>\n",
       "      <td>-3.974554</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641508</td>\n",
       "      <td>1.283017</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>0.005686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 02:00:00</th>\n",
       "      <td>9108.9</td>\n",
       "      <td>9148.8</td>\n",
       "      <td>9075.2</td>\n",
       "      <td>9137.3</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.063914</td>\n",
       "      <td>0.100095</td>\n",
       "      <td>0.098497</td>\n",
       "      <td>-5.231596</td>\n",
       "      <td>0</td>\n",
       "      <td>0.461672</td>\n",
       "      <td>0.923345</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>0.004826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 03:00:00</th>\n",
       "      <td>9137.3</td>\n",
       "      <td>9137.3</td>\n",
       "      <td>9061.5</td>\n",
       "      <td>9082.9</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568728</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>-2.253028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.640387</td>\n",
       "      <td>1.280775</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.005001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 04:00:00</th>\n",
       "      <td>9083.0</td>\n",
       "      <td>9123.4</td>\n",
       "      <td>9060.0</td>\n",
       "      <td>9099.7</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478705</td>\n",
       "      <td>-0.114750</td>\n",
       "      <td>0.069909</td>\n",
       "      <td>-4.428023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.661488</td>\n",
       "      <td>1.322976</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.005546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 07:00:00</th>\n",
       "      <td>87725.2</td>\n",
       "      <td>87900.0</td>\n",
       "      <td>87637.7</td>\n",
       "      <td>87872.6</td>\n",
       "      <td>195.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>-1.149395</td>\n",
       "      <td>0.028068</td>\n",
       "      <td>-1.972040</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.005483</td>\n",
       "      <td>0.003863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 08:00:00</th>\n",
       "      <td>87872.7</td>\n",
       "      <td>87881.8</td>\n",
       "      <td>87342.9</td>\n",
       "      <td>87361.7</td>\n",
       "      <td>232.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014572</td>\n",
       "      <td>-1.115242</td>\n",
       "      <td>0.026972</td>\n",
       "      <td>-6.318691</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.003427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 09:00:00</th>\n",
       "      <td>87353.5</td>\n",
       "      <td>87396.7</td>\n",
       "      <td>86627.9</td>\n",
       "      <td>86776.2</td>\n",
       "      <td>567.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054797</td>\n",
       "      <td>-1.139544</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>-4.989376</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005668</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.004367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 10:00:00</th>\n",
       "      <td>86772.2</td>\n",
       "      <td>86999.9</td>\n",
       "      <td>86595.8</td>\n",
       "      <td>86879.7</td>\n",
       "      <td>195.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108033</td>\n",
       "      <td>-0.985923</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>7.130272</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.003023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 11:00:00</th>\n",
       "      <td>86879.7</td>\n",
       "      <td>86879.7</td>\n",
       "      <td>86818.8</td>\n",
       "      <td>86818.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094859</td>\n",
       "      <td>-0.943593</td>\n",
       "      <td>0.032915</td>\n",
       "      <td>-5.406903</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.002583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53220 rows Ã— 456 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           o        h        l        c  volCcy  \\\n",
       "2019-11-01 00:00:00   9151.2   9155.8   9115.0   9135.0  1174.0   \n",
       "2019-11-01 01:00:00   9135.0   9149.0   9108.8   9108.8   987.0   \n",
       "2019-11-01 02:00:00   9108.9   9148.8   9075.2   9137.3  1655.0   \n",
       "2019-11-01 03:00:00   9137.3   9137.3   9061.5   9082.9  1315.0   \n",
       "2019-11-01 04:00:00   9083.0   9123.4   9060.0   9099.7  1496.0   \n",
       "...                      ...      ...      ...      ...     ...   \n",
       "2025-11-26 07:00:00  87725.2  87900.0  87637.7  87872.6   195.0   \n",
       "2025-11-26 08:00:00  87872.7  87881.8  87342.9  87361.7   232.0   \n",
       "2025-11-26 09:00:00  87353.5  87396.7  86627.9  86776.2   567.0   \n",
       "2025-11-26 10:00:00  86772.2  86999.9  86595.8  86879.7   195.0   \n",
       "2025-11-26 11:00:00  86879.7  86879.7  86818.8  86818.8     5.0   \n",
       "\n",
       "                     time_to_exp1_hr  time_elapsed  hour  day_of_week  \\\n",
       "2019-11-01 00:00:00              7.0          17.0     1            4   \n",
       "2019-11-01 01:00:00              6.0          18.0     2            4   \n",
       "2019-11-01 02:00:00              5.0          19.0     3            4   \n",
       "2019-11-01 03:00:00              4.0          20.0     4            4   \n",
       "2019-11-01 04:00:00              3.0          21.0     5            4   \n",
       "...                              ...           ...   ...          ...   \n",
       "2025-11-26 07:00:00             24.0           0.0     8            2   \n",
       "2025-11-26 08:00:00             23.0           1.0     9            2   \n",
       "2025-11-26 09:00:00             22.0           2.0    10            2   \n",
       "2025-11-26 10:00:00             21.0           3.0    11            2   \n",
       "2025-11-26 11:00:00             20.0           4.0    12            2   \n",
       "\n",
       "                     is_weekend  ...  skew_vol_extreme  kurtosis_vol_extreme  \\\n",
       "2019-11-01 00:00:00           0  ...          1.088773              0.125074   \n",
       "2019-11-01 01:00:00           0  ...          1.117588              0.127607   \n",
       "2019-11-01 02:00:00           0  ...          1.063914              0.100095   \n",
       "2019-11-01 03:00:00           0  ...          0.568728              0.023961   \n",
       "2019-11-01 04:00:00           0  ...          0.478705             -0.114750   \n",
       "...                         ...  ...               ...                   ...   \n",
       "2025-11-26 07:00:00           0  ...          0.013903             -1.149395   \n",
       "2025-11-26 08:00:00           0  ...          0.014572             -1.115242   \n",
       "2025-11-26 09:00:00           0  ...         -0.054797             -1.139544   \n",
       "2025-11-26 10:00:00           0  ...         -0.108033             -0.985923   \n",
       "2025-11-26 11:00:00           0  ...         -0.094859             -0.943593   \n",
       "\n",
       "                     distance_vol_extreme  vol_surprise_clustering  \\\n",
       "2019-11-01 00:00:00              0.121527                -4.691296   \n",
       "2019-11-01 01:00:00              0.122305                -3.974554   \n",
       "2019-11-01 02:00:00              0.098497                -5.231596   \n",
       "2019-11-01 03:00:00              0.064775                -2.253028   \n",
       "2019-11-01 04:00:00              0.069909                -4.428023   \n",
       "...                                   ...                      ...   \n",
       "2025-11-26 07:00:00              0.028068                -1.972040   \n",
       "2025-11-26 08:00:00              0.026972                -6.318691   \n",
       "2025-11-26 09:00:00              0.030126                -4.989376   \n",
       "2025-11-26 10:00:00              0.033643                 7.130272   \n",
       "2025-11-26 11:00:00              0.032915                -5.406903   \n",
       "\n",
       "                     regime_label  max_fwd_z_score  max_jump_z_score  \\\n",
       "2019-11-01 00:00:00             0         0.572082          1.144163   \n",
       "2019-11-01 01:00:00             0         0.641508          1.283017   \n",
       "2019-11-01 02:00:00             0         0.461672          0.923345   \n",
       "2019-11-01 03:00:00             0         0.640387          1.280775   \n",
       "2019-11-01 04:00:00             0         0.661488          1.322976   \n",
       "...                           ...              ...               ...   \n",
       "2025-11-26 07:00:00             0         0.467373          0.000000   \n",
       "2025-11-26 08:00:00             0         0.249501          0.000000   \n",
       "2025-11-26 09:00:00             0         0.092720          0.000000   \n",
       "2025-11-26 10:00:00             0         0.017461          0.000000   \n",
       "2025-11-26 11:00:00          <NA>              NaN               NaN   \n",
       "\n",
       "                     box_std_deseasonalized  box_std_raw  seasonal_vol  \n",
       "2019-11-01 00:00:00                0.006387     0.008368      0.005900  \n",
       "2019-11-01 01:00:00                0.006610     0.008345      0.005686  \n",
       "2019-11-01 02:00:00                0.007804     0.008362      0.004826  \n",
       "2019-11-01 03:00:00                0.007529     0.008361      0.005001  \n",
       "2019-11-01 04:00:00                0.006719     0.008273      0.005546  \n",
       "...                                     ...          ...           ...  \n",
       "2025-11-26 07:00:00                0.006393     0.005483      0.003863  \n",
       "2025-11-26 08:00:00                0.007204     0.005481      0.003427  \n",
       "2025-11-26 09:00:00                0.005668     0.005495      0.004367  \n",
       "2025-11-26 10:00:00                0.008198     0.005503      0.003023  \n",
       "2025-11-26 11:00:00                0.009550     0.005478      0.002583  \n",
       "\n",
       "[53220 rows x 456 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Clean combined dataframe - drop first month and last 11 rows (minimal cleaning for small dataset)\n",
    "months_to_drop = 1  # Only 1 month for small dataset\n",
    "tail_rows_to_drop = 11\n",
    "\n",
    "cutoff = combined_df.index.min() + pd.DateOffset(months=months_to_drop)\n",
    "print(f\"Removing data before {cutoff:%Y-%m-%d} (first {months_to_drop} months)\")\n",
    "combined_df_clean = combined_df.loc[combined_df.index >= cutoff]\n",
    "\n",
    "\n",
    "nan_mask_clean = combined_df_clean.isna().any(axis=1)\n",
    "print(f\"NaN rows after cutoff removal: {nan_mask_clean.sum()} / {combined_df_clean.shape[0]}\")\n",
    "combined_df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if tail_rows_to_drop > 0:\n",
    "    print(f\"Dropping last {tail_rows_to_drop} rows to avoid trailing NaNs\")\n",
    "    combined_df_clean = combined_df_clean.iloc[:-tail_rows_to_drop]\n",
    "\n",
    "print(f\"Rows after cleaning: {len(combined_df_clean)} (from {combined_df_clean.index[0]} to {combined_df_clean.index[-1]})\")\n",
    "\n",
    "# Split into train/val/test (80/10/10)\n",
    "n_samples = len(combined_df_clean)\n",
    "train_end = int(n_samples * 0.8)\n",
    "val_end = train_end + int(n_samples * 0.1)\n",
    "\n",
    "# Get feature and target columns\n",
    "feature_cols = features.columns.intersection(combined_df_clean.columns)\n",
    "target_cols = targets.columns.intersection(combined_df_clean.columns)\n",
    "\n",
    "X_train = combined_df_clean[feature_cols].iloc[:train_end]\n",
    "X_val = combined_df_clean[feature_cols].iloc[train_end:val_end]\n",
    "X_test = combined_df_clean[feature_cols].iloc[val_end:]\n",
    "\n",
    "y_train = combined_df_clean[target_cols].iloc[:train_end]\n",
    "y_val = combined_df_clean[target_cols].iloc[train_end:val_end]\n",
    "y_test = combined_df_clean[target_cols].iloc[val_end:]\n",
    "\n",
    "print(f\"\\nX shapes -> train {X_train.shape}, val {X_val.shape}, test {X_test.shape}\")\n",
    "print(f\"y shapes -> train {y_train.shape}, val {y_val.shape}, test {y_test.shape}\")\n",
    "\n",
    "# Quick NaN check on training data\n",
    "train_nans = X_train.isna().sum()\n",
    "if train_nans.sum() > 0:\n",
    "    print(f\"\\nâš  Training features with NaNs: {(train_nans > 0).sum()} columns\")\n",
    "    print(f\"  Max NaNs in any column: {train_nans.max()} ({train_nans.max()/len(X_train):.1%})\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No NaNs in training features\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

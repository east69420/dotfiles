{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8fd73a",
   "metadata": {},
   "source": [
    "# Feature & Target Pipeline\n",
    "Quick tests and evaluation on new targets/features/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64b95c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and configuration ready\n",
      "=== Loading .hist_db_1h.csv ===\n",
      "\n",
      "Initial rows: 53,963\n",
      "\n",
      "=== FOUND ISSUES (prior to automated fixes) ===\n",
      "\n",
      "ðŸ”´ TEMPORAL: Missing hours: 1 cases\n",
      "  Missing timestamps sample:\n",
      "    2025-11-04 13:00:00\n",
      "\n",
      "ðŸ”´ DATA INTEGRITY: Identical consecutive OHLC rows: 174 cases\n",
      "  Sample cases:\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "    {'o': '7110.10', 'h': '7110.10', 'l': '7110.10', 'c': '7110.10', 'volCcy': '0.00'}\n",
      "  Affected dates (sample): 2020-01-02, 2020-01-03, 2020-01-04, 2020-01-05, 2020-01-06\n",
      "\n",
      "=== APPLYING AUTOMATED FIXES ===\n",
      "ACTION: Resampled/Reindexed to 53964 hourly intervals (was 53963).\n",
      "ACTION: Forward-filled NaNs after resampling. (5 NaNs potentially filled by ffill).\n",
      "\n",
      "=== FINAL STATUS (after automated fixes) ===\n",
      "DataFrame shape post-fixes: (53964, 5) (Original: (53963, 6))\n",
      "Date range: 2019-10-01 00:00:00 to 2025-11-26 11:00:00\n",
      "No null values remaining after fixes.\n",
      "Total null values remaining after fixes: 0\n",
      "Loaded raw data: (53964, 5) in 0.07s\n",
      "Using slice: (53964, 5)\n",
      "Heavy cache ready: heavy_features_v1.pkl (total 1) in cache/heavy_features\n",
      "\n",
      "âœ“ Using heavy cache (only prev_cycle features cached)\n",
      "  Note: Rolling/stateless features still computed on-the-fly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1582: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_sin\"] = df[vol_feat] * df[\"tte_phase_sin\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1584: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_cos\"] = df[vol_feat] * df[\"tte_phase_cos\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1572: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_sqrt\"] = df[vol_feat] * tte_sqrt\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1575: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte\"] = df[vol_feat] * tte\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1578: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_sq\"] = df[vol_feat] * (tte_normalized ** 2)\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1582: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_sin\"] = df[vol_feat] * df[\"tte_phase_sin\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1584: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_x_tte_cos\"] = df[vol_feat] * df[\"tte_phase_cos\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1589: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vol_term_x_tte_sqrt\"] = vol_term_slope * tte_sqrt  # Black-Scholes scaling\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1590: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vol_term_x_tte\"] = vol_term_slope * tte\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1591: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vol_term_x_tte_sq\"] = vol_term_slope * (tte_normalized ** 2)\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1605: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekend\"] = df[vol_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1606: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekday\"] = df[vol_feat] * (1 - df[\"is_weekend\"])\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1605: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekend\"] = df[vol_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1606: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekday\"] = df[vol_feat] * (1 - df[\"is_weekend\"])\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1605: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekend\"] = df[vol_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1606: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekday\"] = df[vol_feat] * (1 - df[\"is_weekend\"])\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1605: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekend\"] = df[vol_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1606: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{vol_feat}_weekday\"] = df[vol_feat] * (1 - df[\"is_weekend\"])\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1616: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{prev_feat}_weekend\"] = df[prev_feat] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1620: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"volume_weekend_effect\"] = df[\"vlm_ma_24h\"] * df[\"is_weekend\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1646: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[interaction_name] = df[prog_feature] * df[vol_feat]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{cycle_type}_range_x_vol\"] = cycle_range * df[\"vol_gkyz_12h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{cycle_type}_range_x_vol\"] = cycle_range * df[\"vol_gkyz_12h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{cycle_type}_range_x_vol\"] = cycle_range * df[\"vol_gkyz_12h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1658: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"weekday_vs_saturday_prog\"] = df[\"prev_weekday_ProgActP\"] - df[\"prev_saturday_ProgActP\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1661: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"weekday_vs_sunday_prog\"] = df[\"prev_weekday_ProgActP\"] - df[\"prev_sunday_ProgActP\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1665: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"prev_cycle_progress_x_hour\"] = df[\"prev_weekday_ProgActP\"] * df[\"hour_of_week_sin\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1674: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"extreme_range_vol\"] = df[\"compressed_range_vol\"] * df[\"vol_gkyz_3h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1678: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"skew_vol_extreme\"] = df[\"returns_skew_24h\"] * df[\"vol_gkyz_6h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1682: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"kurtosis_vol_extreme\"] = df[\"returns_kurtosis_24h\"] * df[\"vol_gkyz_12h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1686: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"distance_vol_extreme\"] = df[\"dist_from_high_144h\"] * df[\"vol_gkyz_24h\"]\n",
      "/shared/eastSync/pyEast/pro_version/featureEngineer.py:1690: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vol_surprise_clustering\"] = df[\"volume_surprise\"] * df[\"vol_clustering\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FeatureEngineer] feature build complete; rows=53964, cols=450, total=10.64s [stateless:234.9ms, merge_stateless:4.3ms, temporal:12.6ms, rolling:762.1ms, prev_week_cycle:44.5ms, current_cycle:9125.5ms, non_linear:250.3ms, custom_interactions:27.6ms, cleanup:177.9ms]\n",
      "  Features computed in 10.64s -> shape: (53964, 450)\n",
      "\n",
      "--- Building Volatility Regime Targets ---\n",
      "Regime targets built in 341.23s -> shape: (53964, 6)\n",
      "Regime targets built in 341.23s -> shape: (53964, 6)\n",
      "\n",
      "Regime distribution:\n",
      "regime_label\n",
      "0    43673\n",
      "1     6364\n",
      "2     3795\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Combined shape: (53964, 456)\n",
      "Total pipeline time: 689.30s\n",
      "\n",
      "Regime distribution:\n",
      "regime_label\n",
      "0    43673\n",
      "1     6364\n",
      "2     3795\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Combined shape: (53964, 456)\n",
      "Total pipeline time: 689.30s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import time\n",
    "from data_pipeline import load_data  # This just loads the data and cleans it\n",
    "from featureEngineer import FeatureEngineer\n",
    "from targetEngineer import ExpirationTargetEngineer\n",
    "from ML_setup import CONFIG\n",
    "from ML_general_tools import *\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Imports and configuration ready\")\n",
    "\n",
    "# Build features, targets, and combined dataframe\n",
    "t0 = time.time()\n",
    "raw_history = load_data(CONFIG[\"data\"][\"path\"])\n",
    "print(f\"Loaded raw data: {raw_history.shape} in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# Use slice for faster testing (or use [:] for full data)\n",
    "history_slice = raw_history[:]  # Last 3000 rows for faster testing\n",
    "print(f\"Using slice: {history_slice.shape}\")\n",
    "\n",
    "feature_params = dict(CONFIG[\"features\"][\"params\"])\n",
    "heavy_cache_cfg = CONFIG[\"features\"].get(\"heavy_cache\", {})\n",
    "heavy_cache_root = Path(heavy_cache_cfg.get(\"directory\", \"cache/heavy_features\"))\n",
    "\n",
    "current_output_root_str = CONFIG[\"output\"][\"directory\"]\n",
    "current_output_root_path = Path(current_output_root_str)\n",
    "\n",
    "paths = {\n",
    "    \"root\": current_output_root_path,\n",
    "    \"feature_selection\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"features\"],\n",
    "    \"trained_models\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"models\"],\n",
    "    \"hpt_studies\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"hpt\"],\n",
    "    \"feature_cache\": current_output_root_path / CONFIG[\"output\"][\"subdirectories\"][\"cache\"]\n",
    "}\n",
    "\n",
    "cache_dir = heavy_cache_root\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "cache_files = sorted(cache_dir.glob(\"heavy_features_v*.pkl\"))\n",
    "cache_ready = bool(cache_files)\n",
    "if cache_ready:\n",
    "    print(f\"Heavy cache ready: {cache_files[-1].name} (total {len(cache_files)}) in {cache_dir}\")\n",
    "else:\n",
    "    print(f\"No heavy cache file found in {cache_dir}; initial fit will populate.\")\n",
    "\n",
    "## Feature Engineering\n",
    "fe = FeatureEngineer(verbose=True, **{k: v for k, v in feature_params.items() if k != \"verbose\"})\n",
    "\n",
    "## Cache usage\n",
    "cache_ready = bool(cache_files)  # Use actual cache status\n",
    "\n",
    "manual_features = None\n",
    "if cache_ready and fe.heavy_cache.load():\n",
    "    print(\"\\nâœ“ Using heavy cache (only prev_cycle features cached)\")\n",
    "    print(\"  Note: Rolling/stateless features still computed on-the-fly\")\n",
    "    t1 = time.time()\n",
    "    fe._heavy_payload = fe.heavy_cache.payload\n",
    "    reference = fe._prepare_reference_frame(history_slice)\n",
    "    fe._full_reference = reference\n",
    "    manual_features = fe._compute_all_features(reference, build_heavy=False)\n",
    "    fe.feature_names_out_ = manual_features.columns.tolist()\n",
    "    fe._reference_features = manual_features\n",
    "    print(f\"  Features computed in {time.time()-t1:.2f}s -> shape: {manual_features.shape}\")\n",
    "else:\n",
    "    print(\"\\nâš  Heavy cache not available; running full fit (slower)\")\n",
    "    t1 = time.time()\n",
    "    verbose_flag = feature_params.pop(\"verbose\", False)\n",
    "    fe = FeatureEngineer(verbose=True, **feature_params)\n",
    "    fe.fit(history_slice)\n",
    "    manual_features = fe.transform(history_slice)\n",
    "    print(f\"  Full fit+transform in {time.time()-t1:.2f}s -> shape: {manual_features.shape}\")\n",
    "\n",
    "feature_engineer = fe\n",
    "features = manual_features.copy()\n",
    "\n",
    "## 2a. Volatility Regime Target Engineering ---\n",
    "from targetEngineer import VolatilityRegimeEngineer\n",
    "\n",
    "print(\"\\n--- Building Volatility Regime Targets ---\")\n",
    "t2 = time.time()\n",
    "\n",
    "regime_engineer = VolatilityRegimeEngineer(\n",
    "    lookback_window=24*3,    # 3 days lookback for vol\n",
    "    seasonal_window=24*30,   # 30 days to learn patterns\n",
    "    forward_window=24,       # 24h classification\n",
    "    trend_std=1.2,           # 1.2 daily sigmas\n",
    "    jump_std=3.0,            # 3.0 daily sigmas\n",
    "    jump_speed_window=6,     # 6h window for jump detection\n",
    ")\n",
    "\n",
    "regime_engineer.fit(features)\n",
    "targets = regime_engineer.transform(features)\n",
    "print(f\"Regime targets built in {time.time()-t2:.2f}s -> shape: {targets.shape}\")\n",
    "\n",
    "# Check distribution\n",
    "dist = regime_engineer.get_regime_distribution(features)\n",
    "print(\"\\nRegime distribution:\")\n",
    "print(dist)\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat([features, targets], axis=1)\n",
    "print(f\"\\nCombined shape: {combined_df.shape}\")\n",
    "print(f\"Total pipeline time: {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cache not found or FORCE_REBUILD=True - will save after first run\n",
      "To use cache next time:\n",
      "  1. Run the first cell with history_slice = raw_history[:]\n",
      "  2. Wait for features/targets to compute\n",
      "  3. This cell will save them\n",
      "  4. Next time, set FORCE_REBUILD=False and skip the first cell\n",
      "============================================================\n",
      "\n",
      "Saving current features and targets to cache...\n",
      "âœ“ Saved to cache in 0.36s\n",
      "  Location: research_vol\n",
      "âœ“ Saved to cache in 0.36s\n",
      "  Location: research_vol\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Define cache paths\n",
    "cache_root = paths[\"root\"]\n",
    "cache_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "feature_cache = cache_root / \"features_cache.pkl\"\n",
    "target_cache = cache_root / \"targets_cache.pkl\"\n",
    "combined_cache = cache_root / \"combined_cache.pkl\"\n",
    "\n",
    "# Option 1: Load from cache if exists\n",
    "FORCE_REBUILD = False  # Set to True to rebuild from scratch\n",
    "\n",
    "if not FORCE_REBUILD and feature_cache.exists() and target_cache.exists():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Loading cached features and targets...\")\n",
    "    t_load = time.time()\n",
    "    \n",
    "    with open(feature_cache, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    with open(target_cache, 'rb') as f:\n",
    "        targets = pickle.load(f)\n",
    "    with open(combined_cache, 'rb') as f:\n",
    "        combined_df = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded from cache in {time.time()-t_load:.2f}s\")\n",
    "    print(f\"  Features: {features.shape}\")\n",
    "    print(f\"  Targets: {targets.shape}\")\n",
    "    print(f\"  Combined: {combined_df.shape}\")\n",
    "    print(f\"  Date range: {features.index[0]} to {features.index[-1]}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Cache not found or FORCE_REBUILD=True - will save after first run\")\n",
    "    print(\"To use cache next time:\")\n",
    "    print(\"  1. Run the first cell with history_slice = raw_history[:]\")\n",
    "    print(\"  2. Wait for features/targets to compute\")\n",
    "    print(\"  3. This cell will save them\")\n",
    "    print(\"  4. Next time, set FORCE_REBUILD=False and skip the first cell\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save the current run to cache\n",
    "    if 'features' in globals() and 'targets' in globals():\n",
    "        print(\"\\nSaving current features and targets to cache...\")\n",
    "        t_save = time.time()\n",
    "        \n",
    "        with open(feature_cache, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "        with open(target_cache, 'wb') as f:\n",
    "            pickle.dump(targets, f)\n",
    "        with open(combined_cache, 'wb') as f:\n",
    "            pickle.dump(combined_df, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved to cache in {time.time()-t_save:.2f}s\")\n",
    "        print(f\"  Location: {cache_root}\")\n",
    "    else:\n",
    "        print(\"âš  No features/targets to save yet - run the first cell first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea14ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NaN Analysis in Features\n",
      "======================================================================\n",
      "\n",
      "Total features: 450\n",
      "Features with NaNs: 207\n",
      "Total rows: 53964\n",
      "\n",
      "======================================================================\n",
      "NaN Features Grouped by Source:\n",
      "======================================================================\n",
      "\n",
      "VOL features: 53 features, 2,697 total NaNs\n",
      "  vol_gkyz_288h_x_tte_sqrt                              288 NaNs ( 0.53%)\n",
      "  vol_gkyz_288h_x_tte                                   288 NaNs ( 0.53%)\n",
      "  vol_gkyz_288h_x_tte_cos                               288 NaNs ( 0.53%)\n",
      "  vol_gkyz_288h_x_tte_sin                               288 NaNs ( 0.53%)\n",
      "  vol_gkyz_288h_x_tte_sq                                288 NaNs ( 0.53%)\n",
      "\n",
      "STOCH features: 7 features, 1,032 total NaNs\n",
      "  stoch_pos_3h                                          173 NaNs ( 0.32%)\n",
      "  stoch_pos_6h                                          169 NaNs ( 0.31%)\n",
      "  stoch_pos_12h                                         162 NaNs ( 0.30%)\n",
      "  stoch_pos_24h                                         156 NaNs ( 0.29%)\n",
      "  stoch_pos_288h                                        144 NaNs ( 0.27%)\n",
      "\n",
      "VLM features: 18 features, 597 total NaNs\n",
      "  vlm_zscore_288h                                       144 NaNs ( 0.27%)\n",
      "  vlm_ma_288h                                           144 NaNs ( 0.27%)\n",
      "  vlm_ma_144h                                            72 NaNs ( 0.13%)\n",
      "  vlm_zscore_144h                                        72 NaNs ( 0.13%)\n",
      "  vlm_zscore_72h                                         36 NaNs ( 0.07%)\n",
      "\n",
      "PRICE features: 7 features, 549 total NaNs\n",
      "  price_rank_288h                                       288 NaNs ( 0.53%)\n",
      "  price_rank_144h                                       144 NaNs ( 0.27%)\n",
      "  price_rank_72h                                         72 NaNs ( 0.13%)\n",
      "  price_rank_24h                                         24 NaNs ( 0.04%)\n",
      "  price_rank_12h                                         12 NaNs ( 0.02%)\n",
      "\n",
      "DIST features: 14 features, 548 total NaNs\n",
      "  dist_from_low_288h                                    144 NaNs ( 0.27%)\n",
      "  dist_from_high_288h                                   144 NaNs ( 0.27%)\n",
      "  dist_from_low_144h                                     72 NaNs ( 0.13%)\n",
      "  dist_from_high_144h                                    72 NaNs ( 0.13%)\n",
      "  dist_from_high_72h                                     36 NaNs ( 0.07%)\n",
      "\n",
      "PREV features: 40 features, 541 total NaNs\n",
      "  prev_saturday_ProgMinP_x_vol24h                        24 NaNs ( 0.04%)\n",
      "  prev_sunday_ProgMaxP_x_vol24h                          24 NaNs ( 0.04%)\n",
      "  prev_sunday_ProgVlm_x_vol24h                           24 NaNs ( 0.04%)\n",
      "  prev_sunday_ProgActP_x_vol24h                          24 NaNs ( 0.04%)\n",
      "  prev_saturday_ProgMaxP_x_vol24h                        24 NaNs ( 0.04%)\n",
      "\n",
      "EXTREME features: 6 features, 154 total NaNs\n",
      "  extreme_prob                                           26 NaNs ( 0.05%)\n",
      "  extreme_prob_x_tte                                     26 NaNs ( 0.05%)\n",
      "  extreme_prob_x_tte_cu                                  26 NaNs ( 0.05%)\n",
      "  extreme_prob_x_tte_sq                                  26 NaNs ( 0.05%)\n",
      "  extreme_prob_x_tte_sqrt                                26 NaNs ( 0.05%)\n",
      "\n",
      "LOGRET features: 11 features, 142 total NaNs\n",
      "  logret_72h                                             73 NaNs ( 0.14%)\n",
      "  logret_24h                                             25 NaNs ( 0.05%)\n",
      "  logret_12h                                             13 NaNs ( 0.02%)\n",
      "  logret_6h                                               7 NaNs ( 0.01%)\n",
      "  logret_5h                                               6 NaNs ( 0.01%)\n",
      "\n",
      "REALIZED features: 2 features, 142 total NaNs\n",
      "  realized_to_expected_24h                               71 NaNs ( 0.13%)\n",
      "  realized_to_expected_tte                               71 NaNs ( 0.13%)\n",
      "\n",
      "DISTANCE features: 1 features, 72 total NaNs\n",
      "  distance_vol_extreme                                   72 NaNs ( 0.13%)\n",
      "\n",
      "======================================================================\n",
      "NaN Location Analysis (Top 10 worst features):\n",
      "======================================================================\n",
      "\n",
      "price_rank_288h:\n",
      "  Total NaNs: 288 (0.53%)\n",
      "  Start NaNs: 287 (before 2019-10-13 00:00:00)\n",
      "  Middle NaNs: 2\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "vol_gkyz_288h_x_tte_sqrt:\n",
      "  Total NaNs: 288 (0.53%)\n",
      "  Start NaNs: 287 (before 2019-10-13 00:00:00)\n",
      "  Middle NaNs: 2\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "vol_gkyz_288h_x_tte:\n",
      "  Total NaNs: 288 (0.53%)\n",
      "  Start NaNs: 287 (before 2019-10-13 00:00:00)\n",
      "  Middle NaNs: 2\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "vol_gkyz_288h_x_tte_cos:\n",
      "  Total NaNs: 288 (0.53%)\n",
      "  Start NaNs: 287 (before 2019-10-13 00:00:00)\n",
      "  Middle NaNs: 2\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "vol_gkyz_288h_x_tte_sin:\n",
      "  Total NaNs: 288 (0.53%)\n",
      "  Start NaNs: 287 (before 2019-10-13 00:00:00)\n",
      "  Middle NaNs: 2\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "vol_gkyz_288h_x_tte_sq:\n",
      "  Total NaNs: 288 (0.53%)\n",
      "  Start NaNs: 287 (before 2019-10-13 00:00:00)\n",
      "  Middle NaNs: 2\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "vol_gkyz_288h:\n",
      "  Total NaNs: 288 (0.53%)\n",
      "  Start NaNs: 287 (before 2019-10-13 00:00:00)\n",
      "  Middle NaNs: 2\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "stoch_pos_3h:\n",
      "  Total NaNs: 173 (0.32%)\n",
      "  Start NaNs: 0 (before 2019-10-01 01:00:00)\n",
      "  Middle NaNs: 174\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "stoch_pos_6h:\n",
      "  Total NaNs: 169 (0.31%)\n",
      "  Start NaNs: 2 (before 2019-10-01 03:00:00)\n",
      "  Middle NaNs: 168\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "stoch_pos_12h:\n",
      "  Total NaNs: 162 (0.30%)\n",
      "  Start NaNs: 5 (before 2019-10-01 06:00:00)\n",
      "  Middle NaNs: 158\n",
      "  End NaNs: -1 (after 2025-11-26 11:00:00)\n",
      "\n",
      "======================================================================\n",
      "Expected NaN Sources (prev_weekend, empirical, etc.):\n",
      "======================================================================\n",
      "\n",
      "prev_saturday/sunday features with NaNs: 26\n",
      "  prev_saturday_ProgMinP_x_vol24h: 24 NaNs\n",
      "  prev_sunday_ProgMaxP_x_vol24h: 24 NaNs\n",
      "  prev_sunday_ProgVlm_x_vol24h: 24 NaNs\n",
      "  prev_sunday_ProgActP_x_vol24h: 24 NaNs\n",
      "  prev_saturday_ProgMaxP_x_vol24h: 24 NaNs\n",
      "\n",
      "emp_ (empirical) features with NaNs: 0\n",
      "\n",
      "prev_weekday features with NaNs: 13\n",
      "  prev_weekday_ProgMinP_x_vol24h: 24 NaNs\n",
      "  prev_weekday_ProgMaxP_x_vol24h: 24 NaNs\n",
      "  prev_weekday_ProgActP_x_vol24h: 24 NaNs\n",
      "  prev_weekday_ProgVlm_x_vol24h: 24 NaNs\n",
      "  prev_weekday_range_x_vol: 12 NaNs\n",
      "\n",
      "======================================================================\n",
      "Row-wise NaN Analysis:\n",
      "======================================================================\n",
      "Rows with ANY NaNs: 460 / 53,964 (0.85%)\n",
      "First row with NaNs: 2019-10-01 00:00:00\n",
      "Last row with NaNs: 2022-12-18 18:00:00\n",
      "Consecutive NaN rows at start: 288\n",
      "Consecutive NaN rows at end: 0\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive NaN analysis in features\n",
    "print(\"=\" * 70)\n",
    "print(\"NaN Analysis in Features\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Overall NaN statistics\n",
    "nan_counts = features.isna().sum()\n",
    "nan_features = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nTotal features: {len(features.columns)}\")\n",
    "print(f\"Features with NaNs: {len(nan_features)}\")\n",
    "print(f\"Total rows: {len(features)}\")\n",
    "\n",
    "# 2. Group NaN features by prefix to identify source\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NaN Features Grouped by Source:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_groups = {}\n",
    "for feat in nan_features.index:\n",
    "    # Extract prefix (everything before first underscore or digit)\n",
    "    if '_' in feat:\n",
    "        prefix = feat.split('_')[0]\n",
    "    else:\n",
    "        prefix = 'other'\n",
    "    \n",
    "    if prefix not in feature_groups:\n",
    "        feature_groups[prefix] = []\n",
    "    feature_groups[prefix].append((feat, nan_counts[feat]))\n",
    "\n",
    "# Sort groups by total NaN count\n",
    "sorted_groups = sorted(feature_groups.items(), \n",
    "                       key=lambda x: sum(count for _, count in x[1]), \n",
    "                       reverse=True)\n",
    "\n",
    "for prefix, features_list in sorted_groups[:10]:  # Top 10 groups\n",
    "    total_nans = sum(count for _, count in features_list)\n",
    "    print(f\"\\n{prefix.upper()} features: {len(features_list)} features, {total_nans:,} total NaNs\")\n",
    "    # Show top 5 within each group\n",
    "    for feat, count in sorted(features_list, key=lambda x: x[1], reverse=True)[:5]:\n",
    "        pct = (count / len(features)) * 100\n",
    "        print(f\"  {feat:50s} {count:6,} NaNs ({pct:5.2f}%)\")\n",
    "\n",
    "# 3. Analyze NaN patterns (start/middle/end)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NaN Location Analysis (Top 10 worst features):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for feat in nan_features.head(10).index:\n",
    "    series = features[feat]\n",
    "    nan_mask = series.isna()\n",
    "    \n",
    "    # Find first and last valid index\n",
    "    valid_indices = series[~nan_mask].index\n",
    "    if len(valid_indices) == 0:\n",
    "        print(f\"\\n{feat}: ALL NaNs!\")\n",
    "        continue\n",
    "    \n",
    "    first_valid = valid_indices[0]\n",
    "    last_valid = valid_indices[-1]\n",
    "    \n",
    "    # Count NaNs at start, middle, end\n",
    "    start_nans = nan_mask.loc[:first_valid].sum() - 1  # -1 to exclude first valid\n",
    "    end_nans = nan_mask.loc[last_valid:].sum() - 1  # -1 to exclude last valid\n",
    "    middle_nans = nan_mask.sum() - start_nans - end_nans\n",
    "    \n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(f\"  Total NaNs: {nan_mask.sum():,} ({nan_mask.sum()/len(features)*100:.2f}%)\")\n",
    "    print(f\"  Start NaNs: {start_nans:,} (before {first_valid})\")\n",
    "    print(f\"  Middle NaNs: {middle_nans:,}\")\n",
    "    print(f\"  End NaNs: {end_nans:,} (after {last_valid})\")\n",
    "\n",
    "# 4. Check specific feature types that are expected\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Expected NaN Sources (prev_weekend, empirical, etc.):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prev_weekend_features = [f for f in nan_features.index if 'prev_saturday' in f or 'prev_sunday' in f]\n",
    "empirical_features = [f for f in nan_features.index if 'emp_' in f]\n",
    "prev_weekday_features = [f for f in nan_features.index if 'prev_weekday' in f]\n",
    "\n",
    "print(f\"\\nprev_saturday/sunday features with NaNs: {len(prev_weekend_features)}\")\n",
    "if prev_weekend_features:\n",
    "    for feat in prev_weekend_features[:5]:\n",
    "        print(f\"  {feat}: {nan_counts[feat]:,} NaNs\")\n",
    "\n",
    "print(f\"\\nemp_ (empirical) features with NaNs: {len(empirical_features)}\")\n",
    "if empirical_features:\n",
    "    for feat in empirical_features[:5]:\n",
    "        print(f\"  {feat}: {nan_counts[feat]:,} NaNs\")\n",
    "\n",
    "print(f\"\\nprev_weekday features with NaNs: {len(prev_weekday_features)}\")\n",
    "if prev_weekday_features:\n",
    "    for feat in prev_weekday_features[:5]:\n",
    "        print(f\"  {feat}: {nan_counts[feat]:,} NaNs\")\n",
    "\n",
    "# 5. Check which rows have NaNs\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Row-wise NaN Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rows_with_nans = features.isna().any(axis=1)\n",
    "print(f\"Rows with ANY NaNs: {rows_with_nans.sum():,} / {len(features):,} ({rows_with_nans.sum()/len(features)*100:.2f}%)\")\n",
    "\n",
    "# Show first and last rows with NaNs\n",
    "nan_row_indices = features[rows_with_nans].index\n",
    "if len(nan_row_indices) > 0:\n",
    "    print(f\"First row with NaNs: {nan_row_indices[0]}\")\n",
    "    print(f\"Last row with NaNs: {nan_row_indices[-1]}\")\n",
    "    \n",
    "    # Count consecutive NaNs at start and end\n",
    "    consecutive_start = 0\n",
    "    for i in range(len(rows_with_nans)):\n",
    "        if rows_with_nans.iloc[i]:\n",
    "            consecutive_start += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    consecutive_end = 0\n",
    "    for i in range(len(rows_with_nans)-1, -1, -1):\n",
    "        if rows_with_nans.iloc[i]:\n",
    "            consecutive_end += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(f\"Consecutive NaN rows at start: {consecutive_start}\")\n",
    "    print(f\"Consecutive NaN rows at end: {consecutive_end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15b86f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Investigating Middle NaNs in Stochastic Features\n",
      "======================================================================\n",
      "\n",
      "Stochastic features: ['stoch_pos_3h', 'stoch_pos_6h', 'stoch_pos_12h', 'stoch_pos_24h', 'stoch_pos_72h', 'stoch_pos_144h', 'stoch_pos_288h']\n",
      "\n",
      "Total NaNs in stoch_pos_3h: 173\n",
      "\n",
      "Date range of NaN occurrences:\n",
      "  First NaN: 2019-10-01 00:00:00\n",
      "  Last NaN: 2022-12-18 18:00:00\n",
      "\n",
      "Sample NaN periods (showing 10 random samples):\n",
      "  2020-01-08 05:00:00\n",
      "  2020-01-04 19:00:00\n",
      "  2020-01-05 16:00:00\n",
      "  2020-01-06 01:00:00\n",
      "  2020-01-08 10:00:00\n",
      "  2020-01-04 13:00:00\n",
      "  2019-10-01 00:00:00\n",
      "  2020-01-07 06:00:00\n",
      "  2020-01-03 15:00:00\n",
      "  2020-01-09 16:00:00\n",
      "\n",
      "======================================================================\n",
      "Checking if NaNs coincide with flat price periods\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['high', 'low', 'close'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m window_start = nan_time - pd.Timedelta(hours=\u001b[32m3\u001b[39m)\n\u001b[32m     41\u001b[39m window_end = nan_time\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m window_data = \u001b[43mraw_history\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwindow_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mwindow_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhigh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlow\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclose\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(window_data) > \u001b[32m0\u001b[39m:\n\u001b[32m     46\u001b[39m     price_range = window_data[\u001b[33m'\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m'\u001b[39m].max() - window_data[\u001b[33m'\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m'\u001b[39m].min()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1185\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m   1184\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m-> \u001b[39m\u001b[32m1185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1187\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[32m   1188\u001b[39m     axis = \u001b[38;5;28mself\u001b[39m.axis \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1378\u001b[39m, in \u001b[36m_LocIndexer._getitem_tuple\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_take_opportunity(tup):\n\u001b[32m   1376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_take(tup)\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1021\u001b[39m, in \u001b[36m_LocationIndexer._getitem_tuple_same_dim\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.is_null_slice(key):\n\u001b[32m   1019\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m retval = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m retval.ndim == \u001b[38;5;28mself\u001b[39m.ndim\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1421\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1419\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1361\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1358\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m   1360\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1363\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1364\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1559\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1556\u001b[39m ax = \u001b[38;5;28mself\u001b[39m.obj._get_axis(axis)\n\u001b[32m   1557\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/eastSync/pyEast/pro_version/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['high', 'low', 'close'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Investigate middle NaNs - specifically stochastic features\n",
    "print(\"=\" * 70)\n",
    "print(\"Investigating Middle NaNs in Stochastic Features\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get stochastic features\n",
    "stoch_features = [col for col in features.columns if 'stoch_pos' in col]\n",
    "print(f\"\\nStochastic features: {stoch_features}\")\n",
    "\n",
    "# Check where NaNs occur\n",
    "stoch_3h = features['stoch_pos_3h']\n",
    "nan_mask = stoch_3h.isna()\n",
    "\n",
    "print(f\"\\nTotal NaNs in stoch_pos_3h: {nan_mask.sum()}\")\n",
    "\n",
    "# Find the NaN rows\n",
    "nan_rows = features[nan_mask]\n",
    "print(f\"\\nDate range of NaN occurrences:\")\n",
    "print(f\"  First NaN: {nan_rows.index[0]}\")\n",
    "print(f\"  Last NaN: {nan_rows.index[-1]}\")\n",
    "\n",
    "# Sample some NaN periods\n",
    "print(f\"\\nSample NaN periods (showing 10 random samples):\")\n",
    "sample_nans = nan_rows.sample(min(10, len(nan_rows)))\n",
    "for idx in sample_nans.index:\n",
    "    print(f\"  {idx}\")\n",
    "\n",
    "# Check if these correspond to flat price periods\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Checking if NaNs coincide with flat price periods\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Need to access raw OHLC data to verify\n",
    "if 'raw_history' in globals():\n",
    "    # Get a few NaN samples and check the raw data around them\n",
    "    sample_nan_times = nan_rows.index[:5]\n",
    "    \n",
    "    for nan_time in sample_nan_times:\n",
    "        # Get 3h window around this time\n",
    "        window_start = nan_time - pd.Timedelta(hours=3)\n",
    "        window_end = nan_time\n",
    "        \n",
    "        window_data = raw_history.loc[window_start:window_end, ['high', 'low', 'close']]\n",
    "        \n",
    "        if len(window_data) > 0:\n",
    "            price_range = window_data['high'].max() - window_data['low'].min()\n",
    "            print(f\"\\n{nan_time}:\")\n",
    "            print(f\"  High-Low range over 3h: ${price_range:.2f}\")\n",
    "            print(f\"  High: ${window_data['high'].max():.2f}, Low: ${window_data['low'].min():.2f}\")\n",
    "            \n",
    "            if price_range < 0.01:\n",
    "                print(f\"  âš ï¸ FLAT PERIOD - price range < $0.01\")\n",
    "else:\n",
    "    print(\"\\nraw_history not available - cannot verify flat periods\")\n",
    "    print(\"But this is the expected cause: high == low over the window â†’ stoch denominator = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8f63c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing data before 2019-11-01 (first 1 months)\n",
      "NaN rows after cutoff removal: 173 / 53220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>c</th>\n",
       "      <th>volCcy</th>\n",
       "      <th>time_to_exp1_hr</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>skew_vol_extreme</th>\n",
       "      <th>kurtosis_vol_extreme</th>\n",
       "      <th>distance_vol_extreme</th>\n",
       "      <th>vol_surprise_clustering</th>\n",
       "      <th>regime_label</th>\n",
       "      <th>max_fwd_z_score</th>\n",
       "      <th>max_jump_z_score</th>\n",
       "      <th>box_std_deseasonalized</th>\n",
       "      <th>box_std_raw</th>\n",
       "      <th>seasonal_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-01 00:00:00</th>\n",
       "      <td>9151.2</td>\n",
       "      <td>9155.8</td>\n",
       "      <td>9115.0</td>\n",
       "      <td>9135.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.088773</td>\n",
       "      <td>0.125074</td>\n",
       "      <td>0.121527</td>\n",
       "      <td>-4.691296</td>\n",
       "      <td>0</td>\n",
       "      <td>0.572082</td>\n",
       "      <td>1.144163</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 01:00:00</th>\n",
       "      <td>9135.0</td>\n",
       "      <td>9149.0</td>\n",
       "      <td>9108.8</td>\n",
       "      <td>9108.8</td>\n",
       "      <td>987.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.117588</td>\n",
       "      <td>0.127607</td>\n",
       "      <td>0.122305</td>\n",
       "      <td>-3.974554</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641508</td>\n",
       "      <td>1.283017</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>0.005686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 02:00:00</th>\n",
       "      <td>9108.9</td>\n",
       "      <td>9148.8</td>\n",
       "      <td>9075.2</td>\n",
       "      <td>9137.3</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.063914</td>\n",
       "      <td>0.100095</td>\n",
       "      <td>0.098497</td>\n",
       "      <td>-5.231596</td>\n",
       "      <td>0</td>\n",
       "      <td>0.461672</td>\n",
       "      <td>0.923345</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>0.004826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 03:00:00</th>\n",
       "      <td>9137.3</td>\n",
       "      <td>9137.3</td>\n",
       "      <td>9061.5</td>\n",
       "      <td>9082.9</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568728</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>-2.253028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.640387</td>\n",
       "      <td>1.280775</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.005001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 04:00:00</th>\n",
       "      <td>9083.0</td>\n",
       "      <td>9123.4</td>\n",
       "      <td>9060.0</td>\n",
       "      <td>9099.7</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478705</td>\n",
       "      <td>-0.114750</td>\n",
       "      <td>0.069909</td>\n",
       "      <td>-4.428023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.661488</td>\n",
       "      <td>1.322976</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.005546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 07:00:00</th>\n",
       "      <td>87725.2</td>\n",
       "      <td>87900.0</td>\n",
       "      <td>87637.7</td>\n",
       "      <td>87872.6</td>\n",
       "      <td>195.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>-1.149395</td>\n",
       "      <td>0.028068</td>\n",
       "      <td>-1.972040</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.005483</td>\n",
       "      <td>0.003863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 08:00:00</th>\n",
       "      <td>87872.7</td>\n",
       "      <td>87881.8</td>\n",
       "      <td>87342.9</td>\n",
       "      <td>87361.7</td>\n",
       "      <td>232.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014572</td>\n",
       "      <td>-1.115242</td>\n",
       "      <td>0.026972</td>\n",
       "      <td>-6.318691</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.003427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 09:00:00</th>\n",
       "      <td>87353.5</td>\n",
       "      <td>87396.7</td>\n",
       "      <td>86627.9</td>\n",
       "      <td>86776.2</td>\n",
       "      <td>567.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054797</td>\n",
       "      <td>-1.139544</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>-4.989376</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005668</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.004367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 10:00:00</th>\n",
       "      <td>86772.2</td>\n",
       "      <td>86999.9</td>\n",
       "      <td>86595.8</td>\n",
       "      <td>86879.7</td>\n",
       "      <td>195.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108033</td>\n",
       "      <td>-0.985923</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>7.130272</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.003023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26 11:00:00</th>\n",
       "      <td>86879.7</td>\n",
       "      <td>86879.7</td>\n",
       "      <td>86818.8</td>\n",
       "      <td>86818.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094859</td>\n",
       "      <td>-0.943593</td>\n",
       "      <td>0.032915</td>\n",
       "      <td>-5.406903</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.002583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53220 rows Ã— 456 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           o        h        l        c  volCcy  \\\n",
       "2019-11-01 00:00:00   9151.2   9155.8   9115.0   9135.0  1174.0   \n",
       "2019-11-01 01:00:00   9135.0   9149.0   9108.8   9108.8   987.0   \n",
       "2019-11-01 02:00:00   9108.9   9148.8   9075.2   9137.3  1655.0   \n",
       "2019-11-01 03:00:00   9137.3   9137.3   9061.5   9082.9  1315.0   \n",
       "2019-11-01 04:00:00   9083.0   9123.4   9060.0   9099.7  1496.0   \n",
       "...                      ...      ...      ...      ...     ...   \n",
       "2025-11-26 07:00:00  87725.2  87900.0  87637.7  87872.6   195.0   \n",
       "2025-11-26 08:00:00  87872.7  87881.8  87342.9  87361.7   232.0   \n",
       "2025-11-26 09:00:00  87353.5  87396.7  86627.9  86776.2   567.0   \n",
       "2025-11-26 10:00:00  86772.2  86999.9  86595.8  86879.7   195.0   \n",
       "2025-11-26 11:00:00  86879.7  86879.7  86818.8  86818.8     5.0   \n",
       "\n",
       "                     time_to_exp1_hr  time_elapsed  hour  day_of_week  \\\n",
       "2019-11-01 00:00:00              7.0          17.0     1            4   \n",
       "2019-11-01 01:00:00              6.0          18.0     2            4   \n",
       "2019-11-01 02:00:00              5.0          19.0     3            4   \n",
       "2019-11-01 03:00:00              4.0          20.0     4            4   \n",
       "2019-11-01 04:00:00              3.0          21.0     5            4   \n",
       "...                              ...           ...   ...          ...   \n",
       "2025-11-26 07:00:00             24.0           0.0     8            2   \n",
       "2025-11-26 08:00:00             23.0           1.0     9            2   \n",
       "2025-11-26 09:00:00             22.0           2.0    10            2   \n",
       "2025-11-26 10:00:00             21.0           3.0    11            2   \n",
       "2025-11-26 11:00:00             20.0           4.0    12            2   \n",
       "\n",
       "                     is_weekend  ...  skew_vol_extreme  kurtosis_vol_extreme  \\\n",
       "2019-11-01 00:00:00           0  ...          1.088773              0.125074   \n",
       "2019-11-01 01:00:00           0  ...          1.117588              0.127607   \n",
       "2019-11-01 02:00:00           0  ...          1.063914              0.100095   \n",
       "2019-11-01 03:00:00           0  ...          0.568728              0.023961   \n",
       "2019-11-01 04:00:00           0  ...          0.478705             -0.114750   \n",
       "...                         ...  ...               ...                   ...   \n",
       "2025-11-26 07:00:00           0  ...          0.013903             -1.149395   \n",
       "2025-11-26 08:00:00           0  ...          0.014572             -1.115242   \n",
       "2025-11-26 09:00:00           0  ...         -0.054797             -1.139544   \n",
       "2025-11-26 10:00:00           0  ...         -0.108033             -0.985923   \n",
       "2025-11-26 11:00:00           0  ...         -0.094859             -0.943593   \n",
       "\n",
       "                     distance_vol_extreme  vol_surprise_clustering  \\\n",
       "2019-11-01 00:00:00              0.121527                -4.691296   \n",
       "2019-11-01 01:00:00              0.122305                -3.974554   \n",
       "2019-11-01 02:00:00              0.098497                -5.231596   \n",
       "2019-11-01 03:00:00              0.064775                -2.253028   \n",
       "2019-11-01 04:00:00              0.069909                -4.428023   \n",
       "...                                   ...                      ...   \n",
       "2025-11-26 07:00:00              0.028068                -1.972040   \n",
       "2025-11-26 08:00:00              0.026972                -6.318691   \n",
       "2025-11-26 09:00:00              0.030126                -4.989376   \n",
       "2025-11-26 10:00:00              0.033643                 7.130272   \n",
       "2025-11-26 11:00:00              0.032915                -5.406903   \n",
       "\n",
       "                     regime_label  max_fwd_z_score  max_jump_z_score  \\\n",
       "2019-11-01 00:00:00             0         0.572082          1.144163   \n",
       "2019-11-01 01:00:00             0         0.641508          1.283017   \n",
       "2019-11-01 02:00:00             0         0.461672          0.923345   \n",
       "2019-11-01 03:00:00             0         0.640387          1.280775   \n",
       "2019-11-01 04:00:00             0         0.661488          1.322976   \n",
       "...                           ...              ...               ...   \n",
       "2025-11-26 07:00:00             0         0.467373          0.000000   \n",
       "2025-11-26 08:00:00             0         0.249501          0.000000   \n",
       "2025-11-26 09:00:00             0         0.092720          0.000000   \n",
       "2025-11-26 10:00:00             0         0.017461          0.000000   \n",
       "2025-11-26 11:00:00          <NA>              NaN               NaN   \n",
       "\n",
       "                     box_std_deseasonalized  box_std_raw  seasonal_vol  \n",
       "2019-11-01 00:00:00                0.006387     0.008368      0.005900  \n",
       "2019-11-01 01:00:00                0.006610     0.008345      0.005686  \n",
       "2019-11-01 02:00:00                0.007804     0.008362      0.004826  \n",
       "2019-11-01 03:00:00                0.007529     0.008361      0.005001  \n",
       "2019-11-01 04:00:00                0.006719     0.008273      0.005546  \n",
       "...                                     ...          ...           ...  \n",
       "2025-11-26 07:00:00                0.006393     0.005483      0.003863  \n",
       "2025-11-26 08:00:00                0.007204     0.005481      0.003427  \n",
       "2025-11-26 09:00:00                0.005668     0.005495      0.004367  \n",
       "2025-11-26 10:00:00                0.008198     0.005503      0.003023  \n",
       "2025-11-26 11:00:00                0.009550     0.005478      0.002583  \n",
       "\n",
       "[53220 rows x 456 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Clean combined dataframe - drop first month and last 11 rows (minimal cleaning for small dataset)\n",
    "months_to_drop = 1  # Only 1 month for small dataset\n",
    "tail_rows_to_drop = 11\n",
    "\n",
    "cutoff = combined_df.index.min() + pd.DateOffset(months=months_to_drop)\n",
    "print(f\"Removing data before {cutoff:%Y-%m-%d} (first {months_to_drop} months)\")\n",
    "combined_df_clean = combined_df.loc[combined_df.index >= cutoff]\n",
    "\n",
    "\n",
    "nan_mask_clean = combined_df_clean.isna().any(axis=1)\n",
    "print(f\"NaN rows after cutoff removal: {nan_mask_clean.sum()} / {combined_df_clean.shape[0]}\")\n",
    "combined_df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if tail_rows_to_drop > 0:\n",
    "    print(f\"Dropping last {tail_rows_to_drop} rows to avoid trailing NaNs\")\n",
    "    combined_df_clean = combined_df_clean.iloc[:-tail_rows_to_drop]\n",
    "\n",
    "print(f\"Rows after cleaning: {len(combined_df_clean)} (from {combined_df_clean.index[0]} to {combined_df_clean.index[-1]})\")\n",
    "\n",
    "# Split into train/val/test (80/10/10)\n",
    "n_samples = len(combined_df_clean)\n",
    "train_end = int(n_samples * 0.8)\n",
    "val_end = train_end + int(n_samples * 0.1)\n",
    "\n",
    "# Get feature and target columns\n",
    "feature_cols = features.columns.intersection(combined_df_clean.columns)\n",
    "target_cols = targets.columns.intersection(combined_df_clean.columns)\n",
    "\n",
    "X_train = combined_df_clean[feature_cols].iloc[:train_end]\n",
    "X_val = combined_df_clean[feature_cols].iloc[train_end:val_end]\n",
    "X_test = combined_df_clean[feature_cols].iloc[val_end:]\n",
    "\n",
    "y_train = combined_df_clean[target_cols].iloc[:train_end]\n",
    "y_val = combined_df_clean[target_cols].iloc[train_end:val_end]\n",
    "y_test = combined_df_clean[target_cols].iloc[val_end:]\n",
    "\n",
    "print(f\"\\nX shapes -> train {X_train.shape}, val {X_val.shape}, test {X_test.shape}\")\n",
    "print(f\"y shapes -> train {y_train.shape}, val {y_val.shape}, test {y_test.shape}\")\n",
    "\n",
    "# Quick NaN check on training data\n",
    "train_nans = X_train.isna().sum()\n",
    "if train_nans.sum() > 0:\n",
    "    print(f\"\\nâš  Training features with NaNs: {(train_nans > 0).sum()} columns\")\n",
    "    print(f\"  Max NaNs in any column: {train_nans.max()} ({train_nans.max()/len(X_train):.1%})\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No NaNs in training features\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

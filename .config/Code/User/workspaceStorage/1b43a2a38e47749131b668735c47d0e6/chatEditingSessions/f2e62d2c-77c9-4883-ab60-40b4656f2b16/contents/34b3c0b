["{\"transientOutputs\":false,\"transientCellMetadata\":{\"breakpointMargin\":true,\"id\":false,\"metadata\":false,\"attachments\":false},\"transientDocumentMetadata\":{\"cells\":true,\"indentAmount\":true},\"cellContentMetadata\":{\"attachments\":true}}","{\"cells\":[{\"cellKind\":1,\"language\":\"markdown\",\"metadata\":{\"metadata\":{},\"id\":\"4d8fd73a\"},\"outputs\":[],\"source\":\"# Feature & Target Pipeline\\nQuick tests and evaluation on new targets/features/models\",\"internalMetadata\":{\"internalId\":\"b6cc7e85\"}},{\"cellKind\":2,\"language\":\"python\",\"metadata\":{\"execution_count\":null,\"id\":\"64b95c13\",\"metadata\":{}},\"outputs\":[{\"outputId\":\"ef7f068c-24cb-4499-993a-effddfeec1a1\",\"metadata\":{\"outputType\":\"stream\",\"scrollable\":true},\"outputs\":[{\"data\":{\"type\":\"ArrayBuffer-4f56482b-5a03-49ba-8356-210d3b0c1c3d\",\"data\":\"SW1wb3J0cyBhbmQgY29uZmlndXJhdGlvbiByZWFkeQo9PT0gTG9hZGluZyAuaGlzdF9kYl8xaC5jc3YgPT09CgpJbml0aWFsIHJvd3M6IDUzLDk2MwoKPT09IEZPVU5EIElTU1VFUyAocHJpb3IgdG8gYXV0b21hdGVkIGZpeGVzKSA9PT0KCvCflLQgVEVNUE9SQUw6IE1pc3NpbmcgaG91cnM6IDEgY2FzZXMKICBNaXNzaW5nIHRpbWVzdGFtcHMgc2FtcGxlOgogICAgMjAyNS0xMS0wNCAxMzowMDowMAoK8J+UtCBEQVRBIElOVEVHUklUWTogSWRlbnRpY2FsIGNvbnNlY3V0aXZlIE9ITEMgcm93czogMTc0IGNhc2VzCiAgU2FtcGxlIGNhc2VzOgogICAgeydvJzogJzcxMTAuMTAnLCAnaCc6ICc3MTEwLjEwJywgJ2wnOiAnNzExMC4xMCcsICdjJzogJzcxMTAuMTAnLCAndm9sQ2N5JzogJzAuMDAnfQogICAgeydvJzogJzcxMTAuMTAnLCAnaCc6ICc3MTEwLjEwJywgJ2wnOiAnNzExMC4xMCcsICdjJzogJzcxMTAuMTAnLCAndm9sQ2N5JzogJzAuMDAnfQogICAgeydvJzogJzcxMTAuMTAnLCAnaCc6ICc3MTEwLjEwJywgJ2wnOiAnNzExMC4xMCcsICdjJzogJzcxMTAuMTAnLCAndm9sQ2N5JzogJzAuMDAnfQogIEFmZmVjdGVkIGRhdGVzIChzYW1wbGUpOiAyMDIwLTAxLTAyLCAyMDIwLTAxLTAzLCAyMDIwLTAxLTA0LCAyMDIwLTAxLTA1LCAyMDIwLTAxLTA2Cgo9PT0gQVBQTFlJTkcgQVVUT01BVEVEIEZJWEVTID09PQpBQ1RJT046IFJlc2FtcGxlZC9SZWluZGV4ZWQgdG8gNTM5NjQgaG91cmx5IGludGVydmFscyAod2FzIDUzOTYzKS4KQUNUSU9OOiBGb3J3YXJkLWZpbGxlZCBOYU5zIGFmdGVyIHJlc2FtcGxpbmcuICg1IE5hTnMgcG90ZW50aWFsbHkgZmlsbGVkIGJ5IGZmaWxsKS4KCj09PSBGSU5BTCBTVEFUVVMgKGFmdGVyIGF1dG9tYXRlZCBmaXhlcykgPT09CkRhdGFGcmFtZSBzaGFwZSBwb3N0LWZpeGVzOiAoNTM5NjQsIDUpIChPcmlnaW5hbDogKDUzOTYzLCA2KSkKRGF0ZSByYW5nZTogMjAxOS0xMC0wMSAwMDowMDowMCB0byAyMDI1LTExLTI2IDExOjAwOjAwCk5vIG51bGwgdmFsdWVzIHJlbWFpbmluZyBhZnRlciBmaXhlcy4KVG90YWwgbnVsbCB2YWx1ZXMgcmVtYWluaW5nIGFmdGVyIGZpeGVzOiAwCkhlYXZ5IGNhY2hlIHJlYWR5OiBoZWF2eV9mZWF0dXJlc192MS5wa2wgKHRvdGFsIDEpIGluIGNhY2hlL2hlYXZ5X2ZlYXR1cmVzCkxvYWRlZCBoZWF2eSBjYWNoZSBwYXlsb2FkIGZyb20gZGlzazsgc2tpcHBpbmcgcmVidWlsZC4KTWFudWFsIGZlYXR1cmVzIHNoYXBlIGZyb20gY2FjaGU6ICg1Mzk2NCwgNDUwKQpNYW51YWwgZmVhdHVyZXMgc2hhcGUgZnJvbSBjYWNoZTogKDUzOTY0LCA0NTApCg==\"},\"mime\":\"application/vnd.code.notebook.stdout\"}]}],\"source\":\"import pandas as pd\\nfrom pathlib import Path\\nfrom typing import Optional\\nfrom data_pipeline import load_data  # This just loads the data and cleans it\\nfrom featureEngineer import FeatureEngineer\\nfrom targetEngineer import ExpirationTargetEngineer\\nfrom ML_setup import CONFIG\\nfrom ML_general_tools import *\\nfrom pathlib import Path\\n\\nprint(\\\"Imports and configuration ready\\\")\\n\\n# Build features, targets, and combined dataframe\\nraw_history = load_data(CONFIG[\\\"data\\\"][\\\"path\\\"])\\nhistory_slice = raw_history[:]\\n\\nfeature_params = dict(CONFIG[\\\"features\\\"][\\\"params\\\"])\\nheavy_cache_cfg = CONFIG[\\\"features\\\"].get(\\\"heavy_cache\\\", {})\\nheavy_cache_root = Path(heavy_cache_cfg.get(\\\"directory\\\", \\\"cache/heavy_features\\\"))\\n\\ncurrent_output_root_str = CONFIG[\\\"output\\\"][\\\"directory\\\"]\\ncurrent_output_root_path = Path(current_output_root_str)\\n\\npaths = {\\n    \\\"root\\\": current_output_root_path,\\n    \\\"feature_selection\\\": current_output_root_path / CONFIG[\\\"output\\\"][\\\"subdirectories\\\"][\\\"features\\\"],\\n    \\\"trained_models\\\": current_output_root_path / CONFIG[\\\"output\\\"][\\\"subdirectories\\\"][\\\"models\\\"],\\n    \\\"hpt_studies\\\": current_output_root_path / CONFIG[\\\"output\\\"][\\\"subdirectories\\\"][\\\"hpt\\\"],\\n    \\\"feature_cache\\\": current_output_root_path / CONFIG[\\\"output\\\"][\\\"subdirectories\\\"][\\\"cache\\\"]\\n}\\n\\n\\ncache_dir = heavy_cache_root\\ncache_dir.mkdir(parents=True, exist_ok=True)\\ncache_files = sorted(cache_dir.glob(\\\"heavy_features_v*.pkl\\\"))\\ncache_ready = bool(cache_files)\\nif cache_ready:\\n    print(f\\\"Heavy cache ready: {cache_files[-1].name} (total {len(cache_files)}) in {cache_dir}\\\")\\nelse:\\n    print(f\\\"No heavy cache file found in {cache_dir}; initial fit will populate.\\\")\\n\\n## Feture Engineering\\nfe = FeatureEngineer(verbose=feature_params.get(\\\"verbose\\\", False), **{k: v for k, v in feature_params.items() if k != \\\"verbose\\\"})\\n\\n## cache\\ncache_ready = True  # Force rebuild for testing\\n\\nmanual_features = None\\nif cache_ready and fe.heavy_cache.load():\\n    print(\\\"Loaded heavy cache payload from disk; skipping rebuild.\\\")\\n    fe._heavy_payload = fe.heavy_cache.payload\\n    reference = fe._prepare_reference_frame(history_slice)\\n    fe._full_reference = reference\\n    manual_features = fe._compute_all_features(reference, build_heavy=False)\\n    fe.feature_names_out_ = manual_features.columns.tolist()\\n    fe._reference_features = manual_features\\n    print(f\\\"Manual features shape from cache: {manual_features.shape}\\\")\\nelse:\\n    print(\\\"Heavy cache not available or failed to load; running full fit.\\\")\\n    verbose_flag = feature_params.pop(\\\"verbose\\\", False)\\n    fe = FeatureEngineer(verbose=verbose_flag, **feature_params)\\n    fe.fit(history_slice)\\n    manual_features = fe.transform(history_slice)\\n\\nfeature_engineer = fe\\nfeatures = manual_features.copy()\\n\\n## Add targets standard expiration targets\\n# target_engineer = ExpirationTargetEngineer(**CONFIG[\\\"targets\\\"][\\\"params\\\"])\\n# target_engineer.fit(features)\\n# targets = target_engineer.transform(features)\\n\\n## 2a. Volatility Regime Target Engineering Test ---\\nfrom targetEngineer import VolatilityRegimeEngineer\\n\\ndf_train = features.copy()\\n\\n# Instantiate with your parameters\\nregime_engineer = VolatilityRegimeEngineer(\\n    lookback_window=24*3,    # 3 days lookback for vol\\n    seasonal_window=24*30,   # 30 days to learn patterns\\n    forward_window=24,       # 24h classification\\n    trend_std=1.2,           # 1.2 daily sigmas\\n    jump_std=3.0,            # 3.0 daily sigmas (scaled internally for 6h window)\\n    jump_speed_window=6,     # 6h window for jump detection\\n\\n    # Hardening Parameters\\n    trend_min_efficiency=0.15, # Allows looser/messier trends\\n    trend_min_r2=0.6           # Requires moderate linear fit\\n)\\n\\n# Run Fit/Transform\\nregime_engineer.fit(df_train)\\ntargets = regime_engineer.transform(df_train)\\n\\n# Check the distribution\\ndist = regime_engineer.get_regime_distribution(df_train)\\nprint(dist)\\n\\n\\ninitial_feature_names = list(features.columns)\\n\\n# Generate targets (your existing logic)\\nprint(initial_feature_names)\\n\\n\\n## deop price columns from features\\n# drop_cols = [col for col in ['o', 'h', 'l', 'c', 'volCcy'] if col in features.columns]\\n# if drop_cols: features = features.drop(columns=drop_cols)\\n\\n\\n# Combine\\ncombined_df = pd.concat([features, targets], axis=1)\\n\",\"internalMetadata\":{\"executionOrder\":3,\"executionId\":\"7e9fbc2e-6262-42d9-999a-ed3f77899505\",\"runStartTime\":1764178546918,\"renderDuration\":{\"vscode.builtin-renderer\":2},\"runStartTimeAdjustment\":0,\"internalId\":\"d9559027\"}},{\"cellKind\":2,\"language\":\"python\",\"metadata\":{\"execution_count\":1,\"id\":\"030f489c\",\"metadata\":{}},\"outputs\":[{\"outputId\":\"49251b87-6527-4753-80ce-d9f7c4cb442d\",\"metadata\":{\"outputType\":\"error\",\"originalError\":{\"output_type\":\"error\",\"ename\":\"NameError\",\"evalue\":\"name 'features' is not defined\",\"traceback\":[\"\\u001b[31m---------------------------------------------------------------------------\\u001b[39m\",\"\\u001b[31mNameError\\u001b[39m                                 Traceback (most recent call last)\",\"\\u001b[36mCell\\u001b[39m\\u001b[36m \\u001b[39m\\u001b[32mIn[1]\\u001b[39m\\u001b[32m, line 2\\u001b[39m\\n\\u001b[32m      1\\u001b[39m \\u001b[38;5;66;03m# Find rows with any NaN values in features\\u001b[39;00m\\n\\u001b[32m----> \\u001b[39m\\u001b[32m2\\u001b[39m nan_mask = \\u001b[43mfeatures\\u001b[49m.isna().any(axis=\\u001b[32m1\\u001b[39m)\\n\\u001b[32m      3\\u001b[39m features_with_nans = features[nan_mask]\\n\",\"\\u001b[31mNameError\\u001b[39m: name 'features' is not defined\"]}},\"outputs\":[{\"data\":{\"type\":\"ArrayBuffer-4f56482b-5a03-49ba-8356-210d3b0c1c3d\",\"data\":\"ewoJIm5hbWUiOiAiTmFtZUVycm9yIiwKCSJtZXNzYWdlIjogIm5hbWUgJ2ZlYXR1cmVzJyBpcyBub3QgZGVmaW5lZCIsCgkic3RhY2siOiAiXHUwMDFiWzMxbS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLVx1MDAxYlszOW1cblx1MDAxYlszMW1OYW1lRXJyb3JcdTAwMWJbMzltICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgVHJhY2ViYWNrIChtb3N0IHJlY2VudCBjYWxsIGxhc3QpXG5cdTAwMWJbMzZtQ2VsbFx1MDAxYlszOW1cdTAwMWJbMzZtIFx1MDAxYlszOW1cdTAwMWJbMzJtSW5bMV1cdTAwMWJbMzltXHUwMDFiWzMybSwgbGluZSAyXHUwMDFiWzM5bVxuXHUwMDFiWzMybSAgICAgIDFcdTAwMWJbMzltIFx1MDAxYlszODs1OzY2OzAzbSMgRmluZCByb3dzIHdpdGggYW55IE5hTiB2YWx1ZXMgaW4gZmVhdHVyZXNcdTAwMWJbMzk7MDBtXG5cdTAwMWJbMzJtLS0tLT4gXHUwMDFiWzM5bVx1MDAxYlszMm0yXHUwMDFiWzM5bSBuYW5fbWFzayA9IFx1MDAxYls0M21mZWF0dXJlc1x1MDAxYls0OW0uaXNuYSgpLmFueShheGlzPVx1MDAxYlszMm0xXHUwMDFiWzM5bSlcblx1MDAxYlszMm0gICAgICAzXHUwMDFiWzM5bSBmZWF0dXJlc193aXRoX25hbnMgPSBmZWF0dXJlc1tuYW5fbWFza11cblxuXHUwMDFiWzMxbU5hbWVFcnJvclx1MDAxYlszOW06IG5hbWUgJ2ZlYXR1cmVzJyBpcyBub3QgZGVmaW5lZCIKfQ==\"},\"mime\":\"application/vnd.code.notebook.error\"}]}],\"mime\":\"text/plain\",\"source\":\"\\n# Find rows with any NaN values in features\\nnan_mask = features.isna().any(axis=1)\\nfeatures_with_nans = features[nan_mask]\\n\\n\",\"internalMetadata\":{\"internalId\":\"8d03f25f\",\"runStartTimeAdjustment\":0,\"executionId\":\"4b5a9213-29e4-4548-90bd-298b8efc8811\",\"runStartTime\":1764178353998,\"runEndTime\":1764178354110,\"lastRunSuccess\":false,\"executionOrder\":1,\"renderDuration\":{\"vscode.builtin-renderer\":1},\"error\":{\"name\":\"NameError\",\"message\":\"name 'features' is not defined\",\"stack\":\"\\u001b[31m---------------------------------------------------------------------------\\u001b[39m\\n\\u001b[31mNameError\\u001b[39m                                 Traceback (most recent call last)\\n\\u001b[36mCell\\u001b[39m\\u001b[36m \\u001b[39m\\u001b[32mIn[1]\\u001b[39m\\u001b[32m, line 2\\u001b[39m\\n\\u001b[32m      1\\u001b[39m \\u001b[38;5;66;03m# Find rows with any NaN values in features\\u001b[39;00m\\n\\u001b[32m----> \\u001b[39m\\u001b[32m2\\u001b[39m nan_mask = \\u001b[43mfeatures\\u001b[49m.isna().any(axis=\\u001b[32m1\\u001b[39m)\\n\\u001b[32m      3\\u001b[39m features_with_nans = features[nan_mask]\\n\\n\\u001b[31mNameError\\u001b[39m: name 'features' is not defined\",\"location\":{\"startLineNumber\":1,\"startColumn\":0,\"endLineNumber\":1,\"endColumn\":0},\"uri\":{\"$mid\":1,\"fsPath\":\"/home/east/shared/eastSync/pyEast/pro_version/targeting_volatility.ipynb\",\"external\":\"vscode-notebook-cell:/home/east/shared/eastSync/pyEast/pro_version/targeting_volatility.ipynb#X12sZmlsZQ%3D%3D\",\"path\":\"/home/east/shared/eastSync/pyEast/pro_version/targeting_volatility.ipynb\",\"scheme\":\"vscode-notebook-cell\",\"fragment\":\"X12sZmlsZQ==\"}}}},{\"cellKind\":2,\"language\":\"python\",\"metadata\":{\"execution_count\":null,\"id\":\"aea14ab6\",\"metadata\":{}},\"outputs\":[],\"mime\":\"text/plain\",\"source\":\"\\n\\n# Clean combined dataframe - drop first month and last 11 rows (minimal cleaning for small dataset)\\nmonths_to_drop = 1  # Only 1 month for small dataset\\ntail_rows_to_drop = 11\\n\\ncutoff = combined_df.index.min() + pd.DateOffset(months=months_to_drop)\\nprint(f\\\"Removing data before {cutoff:%Y-%m-%d} (first {months_to_drop} months)\\\")\\ncombined_df_clean = combined_df.loc[combined_df.index >= cutoff]\\n\\nif tail_rows_to_drop > 0:\\n    print(f\\\"Dropping last {tail_rows_to_drop} rows to avoid trailing NaNs\\\")\\n    combined_df_clean = combined_df_clean.iloc[:-tail_rows_to_drop]\\n\\nprint(f\\\"Rows after cleaning: {len(combined_df_clean)} (from {combined_df_clean.index[0]} to {combined_df_clean.index[-1]})\\\")\\n\\n# Split into train/val/test (80/10/10)\\nn_samples = len(combined_df_clean)\\ntrain_end = int(n_samples * 0.8)\\nval_end = train_end + int(n_samples * 0.1)\\n\\n# Get feature and target columns\\nfeature_cols = features.columns.intersection(combined_df_clean.columns)\\ntarget_cols = targets.columns.intersection(combined_df_clean.columns)\\n\\nX_train = combined_df_clean[feature_cols].iloc[:train_end]\\nX_val = combined_df_clean[feature_cols].iloc[train_end:val_end]\\nX_test = combined_df_clean[feature_cols].iloc[val_end:]\\n\\ny_train = combined_df_clean[target_cols].iloc[:train_end]\\ny_val = combined_df_clean[target_cols].iloc[train_end:val_end]\\ny_test = combined_df_clean[target_cols].iloc[val_end:]\\n\\nprint(f\\\"\\\\nX shapes -> train {X_train.shape}, val {X_val.shape}, test {X_test.shape}\\\")\\nprint(f\\\"y shapes -> train {y_train.shape}, val {y_val.shape}, test {y_test.shape}\\\")\\n\\n# Quick NaN check on training data\\ntrain_nans = X_train.isna().sum()\\nif train_nans.sum() > 0:\\n    print(f\\\"\\\\n⚠ Training features with NaNs: {(train_nans > 0).sum()} columns\\\")\\n    print(f\\\"  Max NaNs in any column: {train_nans.max()} ({train_nans.max()/len(X_train):.1%})\\\")\\nelse:\\n    print(\\\"\\\\n✓ No NaNs in training features\\\")\\n\",\"internalMetadata\":{\"internalId\":\"8809a7a5\"}}],\"metadata\":{\"metadata\":{\"kernelspec\":{\"display_name\":\".venv\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"file_extension\":\".py\",\"mimetype\":\"text/x-python\",\"name\":\"python\",\"nbconvert_exporter\":\"python\",\"pygments_lexer\":\"ipython3\",\"version\":\"3.13.7\"}},\"nbformat\":4,\"nbformat_minor\":5}}"]